# PIPELINE DEFINITION
# Name: docling-convert-pipeline
# Description: Converts PDF documents in a git repository to Markdown using Docling and generates embeddings
# Inputs:
#    base_url: str [Default: 'https://raw.githubusercontent.com/docling-project/docling/main/tests/data/pdf']
#    embed_model_id: str [Default: 'ibm-granite/granite-embedding-125m-english']
#    max_tokens: int [Default: 512.0]
#    num_workers: int [Default: 1.0]
#    pdf_filenames: str [Default: '2203.01017v2.pdf, 2206.01062.pdf, 2305.03393v1-pg9.pdf, amt_handbook_sample.pdf, code_and_formula.pdf, multi_page.pdf, picture_classification.pdf, redp5110_sampled.pdf, right_to_left_01.pdf, right_to_left_02.pdf, right_to_left_03.pdf']
#    service_url: str [Default: 'http://lsd-llama-milvus-service:8321']
#    use_gpu: bool [Default: True]
#    vector_db_id: str [Default: 'my_demo_vector_id']
components:
  comp-condition-3:
    dag:
      tasks:
        docling-convert:
          cachingOptions: {}
          componentRef:
            name: comp-docling-convert
          inputs:
            artifacts:
              input_path:
                componentInputArtifact: pipelinechannel--import-test-pdfs-output_path
            parameters:
              embed_model_id:
                componentInputParameter: pipelinechannel--embed_model_id
              max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              pdf_split:
                componentInputParameter: pipelinechannel--create-pdf-splits-Output-loop-item
              service_url:
                componentInputParameter: pipelinechannel--service_url
              vector_db_id:
                componentInputParameter: pipelinechannel--vector_db_id
          taskInfo:
            name: docling-convert
    inputDefinitions:
      artifacts:
        pipelinechannel--import-test-pdfs-output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--create-pdf-splits-Output-loop-item:
          parameterType: LIST
        pipelinechannel--embed_model_id:
          parameterType: STRING
        pipelinechannel--max_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--service_url:
          parameterType: STRING
        pipelinechannel--use_gpu:
          parameterType: BOOLEAN
        pipelinechannel--vector_db_id:
          parameterType: STRING
  comp-condition-4:
    dag:
      tasks:
        docling-convert-2:
          cachingOptions: {}
          componentRef:
            name: comp-docling-convert-2
          inputs:
            artifacts:
              input_path:
                componentInputArtifact: pipelinechannel--import-test-pdfs-output_path
            parameters:
              embed_model_id:
                componentInputParameter: pipelinechannel--embed_model_id
              max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              pdf_split:
                componentInputParameter: pipelinechannel--create-pdf-splits-Output-loop-item
              service_url:
                componentInputParameter: pipelinechannel--service_url
              vector_db_id:
                componentInputParameter: pipelinechannel--vector_db_id
          taskInfo:
            name: docling-convert-2
    inputDefinitions:
      artifacts:
        pipelinechannel--import-test-pdfs-output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--create-pdf-splits-Output-loop-item:
          parameterType: LIST
        pipelinechannel--embed_model_id:
          parameterType: STRING
        pipelinechannel--max_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--service_url:
          parameterType: STRING
        pipelinechannel--use_gpu:
          parameterType: BOOLEAN
        pipelinechannel--vector_db_id:
          parameterType: STRING
  comp-condition-branches-2:
    dag:
      tasks:
        condition-3:
          componentRef:
            name: comp-condition-3
          inputs:
            artifacts:
              pipelinechannel--import-test-pdfs-output_path:
                componentInputArtifact: pipelinechannel--import-test-pdfs-output_path
            parameters:
              pipelinechannel--create-pdf-splits-Output-loop-item:
                componentInputParameter: pipelinechannel--create-pdf-splits-Output-loop-item
              pipelinechannel--embed_model_id:
                componentInputParameter: pipelinechannel--embed_model_id
              pipelinechannel--max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              pipelinechannel--service_url:
                componentInputParameter: pipelinechannel--service_url
              pipelinechannel--use_gpu:
                componentInputParameter: pipelinechannel--use_gpu
              pipelinechannel--vector_db_id:
                componentInputParameter: pipelinechannel--vector_db_id
          taskInfo:
            name: condition-3
          triggerPolicy:
            condition: inputs.parameter_values['pipelinechannel--use_gpu'] == true
        condition-4:
          componentRef:
            name: comp-condition-4
          inputs:
            artifacts:
              pipelinechannel--import-test-pdfs-output_path:
                componentInputArtifact: pipelinechannel--import-test-pdfs-output_path
            parameters:
              pipelinechannel--create-pdf-splits-Output-loop-item:
                componentInputParameter: pipelinechannel--create-pdf-splits-Output-loop-item
              pipelinechannel--embed_model_id:
                componentInputParameter: pipelinechannel--embed_model_id
              pipelinechannel--max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              pipelinechannel--service_url:
                componentInputParameter: pipelinechannel--service_url
              pipelinechannel--use_gpu:
                componentInputParameter: pipelinechannel--use_gpu
              pipelinechannel--vector_db_id:
                componentInputParameter: pipelinechannel--vector_db_id
          taskInfo:
            name: condition-4
          triggerPolicy:
            condition: '!(inputs.parameter_values[''pipelinechannel--use_gpu''] ==
              true)'
    inputDefinitions:
      artifacts:
        pipelinechannel--import-test-pdfs-output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--create-pdf-splits-Output-loop-item:
          parameterType: LIST
        pipelinechannel--embed_model_id:
          parameterType: STRING
        pipelinechannel--max_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--service_url:
          parameterType: STRING
        pipelinechannel--use_gpu:
          parameterType: BOOLEAN
        pipelinechannel--vector_db_id:
          parameterType: STRING
  comp-create-pdf-splits:
    executorLabel: exec-create-pdf-splits
    inputDefinitions:
      artifacts:
        input_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        num_splits:
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      parameters:
        Output:
          parameterType: LIST
  comp-docling-convert:
    executorLabel: exec-docling-convert
    inputDefinitions:
      artifacts:
        input_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        embed_model_id:
          parameterType: STRING
        max_tokens:
          parameterType: NUMBER_INTEGER
        pdf_split:
          parameterType: LIST
        service_url:
          parameterType: STRING
        vector_db_id:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-docling-convert-2:
    executorLabel: exec-docling-convert-2
    inputDefinitions:
      artifacts:
        input_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        embed_model_id:
          parameterType: STRING
        max_tokens:
          parameterType: NUMBER_INTEGER
        pdf_split:
          parameterType: LIST
        service_url:
          parameterType: STRING
        vector_db_id:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-for-loop-1:
    dag:
      tasks:
        condition-branches-2:
          componentRef:
            name: comp-condition-branches-2
          inputs:
            artifacts:
              pipelinechannel--import-test-pdfs-output_path:
                componentInputArtifact: pipelinechannel--import-test-pdfs-output_path
            parameters:
              pipelinechannel--create-pdf-splits-Output-loop-item:
                componentInputParameter: pipelinechannel--create-pdf-splits-Output-loop-item
              pipelinechannel--embed_model_id:
                componentInputParameter: pipelinechannel--embed_model_id
              pipelinechannel--max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              pipelinechannel--service_url:
                componentInputParameter: pipelinechannel--service_url
              pipelinechannel--use_gpu:
                componentInputParameter: pipelinechannel--use_gpu
              pipelinechannel--vector_db_id:
                componentInputParameter: pipelinechannel--vector_db_id
          taskInfo:
            name: condition-branches-2
    inputDefinitions:
      artifacts:
        pipelinechannel--import-test-pdfs-output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--create-pdf-splits-Output:
          parameterType: LIST
        pipelinechannel--create-pdf-splits-Output-loop-item:
          parameterType: LIST
        pipelinechannel--embed_model_id:
          parameterType: STRING
        pipelinechannel--max_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--service_url:
          parameterType: STRING
        pipelinechannel--use_gpu:
          parameterType: BOOLEAN
        pipelinechannel--vector_db_id:
          parameterType: STRING
  comp-import-test-pdfs:
    executorLabel: exec-import-test-pdfs
    inputDefinitions:
      parameters:
        base_url:
          parameterType: STRING
        pdf_filenames:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        output_path:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-register-vector-db:
    executorLabel: exec-register-vector-db
    inputDefinitions:
      parameters:
        embed_model_id:
          parameterType: STRING
        service_url:
          parameterType: STRING
        vector_db_id:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-create-pdf-splits:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - create_pdf_splits
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef create_pdf_splits(\n    input_path: dsl.InputPath(\"input-pdfs\"\
          ),\n    num_splits: int,\n) -> List[List[str]]:\n    import pathlib\n\n\
          \    # Split our entire directory of pdfs into n batches, where n == num_splits\n\
          \    all_pdfs = [path.name for path in pathlib.Path(input_path).glob(\"\
          *.pdf\")]\n    splits = [\n        batch for batch in (all_pdfs[i::num_splits]\
          \ for i in range(num_splits)) if batch\n    ]\n    return splits or [[]]\n\
          \n"
        image: registry.redhat.io/ubi9/python-312@sha256:e80ff3673c95b91f0dafdbe97afb261eab8244d7fd8b47e20ffcbcfee27fb168
    exec-docling-convert:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - docling_convert
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'docling>=2.43.0'\
          \ 'transformers' 'sentence-transformers' 'llama-stack' 'llama-stack-client'\
          \ 'pymilvus' 'fire' 'rapidocr-onnxruntime' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef docling_convert(\n    input_path: dsl.InputPath(\"input-pdfs\"\
          ),\n    pdf_split: List[str],\n    output_path: dsl.OutputPath(\"output-md\"\
          ),\n    embed_model_id: str,\n    max_tokens: int,\n    service_url: str,\n\
          \    vector_db_id: str,\n):\n    import pathlib\n\n    from docling.datamodel.base_models\
          \ import InputFormat, ConversionStatus\n    from docling.datamodel.pipeline_options\
          \ import PdfPipelineOptions, RapidOcrOptions\n    from docling.document_converter\
          \ import DocumentConverter, PdfFormatOption\n    from transformers import\
          \ AutoTokenizer\n    from sentence_transformers import SentenceTransformer\n\
          \    from docling.chunking import HybridChunker\n    import logging\n  \
          \  from llama_stack_client import LlamaStackClient\n    import uuid\n\n\
          \    import json\n\n    _log = logging.getLogger(__name__)\n\n    # ----\
          \ Helper functions ----\n    def setup_chunker_and_embedder(embed_model_id:\
          \ str, max_tokens: int):\n        tokenizer = AutoTokenizer.from_pretrained(embed_model_id)\n\
          \        embedding_model = SentenceTransformer(embed_model_id)\n       \
          \ chunker = HybridChunker(\n            tokenizer=tokenizer, max_tokens=max_tokens,\
          \ merge_peers=True\n        )\n        return embedding_model, chunker\n\
          \n    def embed_text(text: str, embedding_model) -> list[float]:\n     \
          \   return embedding_model.encode([text], normalize_embeddings=True).tolist()[0]\n\
          \n    def process_and_insert_embeddings(conv_results):\n        processed_docs\
          \ = 0\n        for conv_res in conv_results:\n            if conv_res.status\
          \ != ConversionStatus.SUCCESS:\n                _log.warning(\n        \
          \            f\"Conversion failed for {conv_res.input.file.stem}: {conv_res.status}\"\
          \n                )\n                continue\n\n            processed_docs\
          \ += 1\n            file_name = conv_res.input.file.stem\n            document\
          \ = conv_res.document\n\n            if document is None:\n            \
          \    _log.warning(f\"Document conversion failed for {file_name}\")\n   \
          \             continue\n\n            embedding_model, chunker = setup_chunker_and_embedder(\n\
          \                embed_model_id, max_tokens\n            )\n\n         \
          \   chunks_with_embedding = []\n            for chunk in chunker.chunk(dl_doc=document):\n\
          \                raw_chunk = chunker.contextualize(chunk)\n            \
          \    embedding = embed_text(raw_chunk, embedding_model)\n\n            \
          \    chunk_id = str(uuid.uuid4())  # Generate a unique ID for the chunk\n\
          \                content_token_count = chunker.tokenizer.count_tokens(raw_chunk)\n\
          \n                # Prepare metadata object\n                metadata_obj\
          \ = {\n                    \"file_name\": file_name,\n                 \
          \   \"document_id\": chunk_id,\n                    \"token_count\": content_token_count,\n\
          \                }\n\n                metadata_str = json.dumps(metadata_obj)\n\
          \                metadata_token_count = chunker.tokenizer.count_tokens(metadata_str)\n\
          \                metadata_obj[\"metadata_token_count\"] = metadata_token_count\n\
          \n                chunks_with_embedding.append(\n                    {\n\
          \                        \"content\": raw_chunk,\n                     \
          \   \"mime_type\": \"text/markdown\",\n                        \"embedding\"\
          : embedding,\n                        \"metadata\": metadata_obj,\n    \
          \                }\n                )\n            if chunks_with_embedding:\n\
          \                try:\n                    client.vector_io.insert(\n  \
          \                      vector_db_id=vector_db_id, chunks=chunks_with_embedding\n\
          \                    )\n                except Exception as e:\n       \
          \             _log.error(f\"Failed to insert embeddings into vector database:\
          \ {e}\")\n\n        _log.info(f\"Processed {processed_docs} documents successfully.\"\
          )\n\n    # ---- Main logic ----\n    input_path = pathlib.Path(input_path)\n\
          \    output_path = pathlib.Path(output_path)\n    output_path.mkdir(parents=True,\
          \ exist_ok=True)\n\n    # Original code using splits\n    input_pdfs = [input_path\
          \ / name for name in pdf_split]\n    # Alternative not using splits\n  \
          \  # input_pdfs = pathlib.Path(input_path).glob(\"*.pdf\")\n\n    # Required\
          \ models are automatically downloaded when they are\n    # not provided\
          \ in PdfPipelineOptions initialization\n    pipeline_options = PdfPipelineOptions()\n\
          \    pipeline_options.do_ocr = True\n    pipeline_options.generate_page_images\
          \ = True\n    pipeline_options.ocr_options = RapidOcrOptions()\n\n    doc_converter\
          \ = DocumentConverter(\n        format_options={\n            InputFormat.PDF:\
          \ PdfFormatOption(pipeline_options=pipeline_options)\n        }\n    )\n\
          \n    conv_results = doc_converter.convert_all(\n        input_pdfs,\n \
          \       raises_on_error=True,\n    )\n\n    # Initialize LlamaStack client\n\
          \    client = LlamaStackClient(base_url=service_url)\n\n    # Process the\
          \ conversion results and insert embeddings into the vector database\n  \
          \  process_and_insert_embeddings(conv_results)\n\n"
        image: quay.io/modh/odh-pipeline-runtime-pytorch-cuda-py311-ubi9@sha256:4706be608af3f33c88700ef6ef6a99e716fc95fc7d2e879502e81c0022fd840e
        resources:
          accelerator:
            count: '1'
            resourceCount: '1'
            resourceType: nvidia.com/gpu
            type: nvidia.com/gpu
          cpuLimit: 4.0
          cpuRequest: 0.5
          memoryLimit: 6.442450944
          memoryRequest: 2.147483648
          resourceCpuLimit: '4'
          resourceCpuRequest: 500m
          resourceMemoryLimit: 6Gi
          resourceMemoryRequest: 2Gi
    exec-docling-convert-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - docling_convert
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'docling>=2.43.0'\
          \ 'transformers' 'sentence-transformers' 'llama-stack' 'llama-stack-client'\
          \ 'pymilvus' 'fire' 'rapidocr-onnxruntime' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef docling_convert(\n    input_path: dsl.InputPath(\"input-pdfs\"\
          ),\n    pdf_split: List[str],\n    output_path: dsl.OutputPath(\"output-md\"\
          ),\n    embed_model_id: str,\n    max_tokens: int,\n    service_url: str,\n\
          \    vector_db_id: str,\n):\n    import pathlib\n\n    from docling.datamodel.base_models\
          \ import InputFormat, ConversionStatus\n    from docling.datamodel.pipeline_options\
          \ import PdfPipelineOptions, RapidOcrOptions\n    from docling.document_converter\
          \ import DocumentConverter, PdfFormatOption\n    from transformers import\
          \ AutoTokenizer\n    from sentence_transformers import SentenceTransformer\n\
          \    from docling.chunking import HybridChunker\n    import logging\n  \
          \  from llama_stack_client import LlamaStackClient\n    import uuid\n\n\
          \    import json\n\n    _log = logging.getLogger(__name__)\n\n    # ----\
          \ Helper functions ----\n    def setup_chunker_and_embedder(embed_model_id:\
          \ str, max_tokens: int):\n        tokenizer = AutoTokenizer.from_pretrained(embed_model_id)\n\
          \        embedding_model = SentenceTransformer(embed_model_id)\n       \
          \ chunker = HybridChunker(\n            tokenizer=tokenizer, max_tokens=max_tokens,\
          \ merge_peers=True\n        )\n        return embedding_model, chunker\n\
          \n    def embed_text(text: str, embedding_model) -> list[float]:\n     \
          \   return embedding_model.encode([text], normalize_embeddings=True).tolist()[0]\n\
          \n    def process_and_insert_embeddings(conv_results):\n        processed_docs\
          \ = 0\n        for conv_res in conv_results:\n            if conv_res.status\
          \ != ConversionStatus.SUCCESS:\n                _log.warning(\n        \
          \            f\"Conversion failed for {conv_res.input.file.stem}: {conv_res.status}\"\
          \n                )\n                continue\n\n            processed_docs\
          \ += 1\n            file_name = conv_res.input.file.stem\n            document\
          \ = conv_res.document\n\n            if document is None:\n            \
          \    _log.warning(f\"Document conversion failed for {file_name}\")\n   \
          \             continue\n\n            embedding_model, chunker = setup_chunker_and_embedder(\n\
          \                embed_model_id, max_tokens\n            )\n\n         \
          \   chunks_with_embedding = []\n            for chunk in chunker.chunk(dl_doc=document):\n\
          \                raw_chunk = chunker.contextualize(chunk)\n            \
          \    embedding = embed_text(raw_chunk, embedding_model)\n\n            \
          \    chunk_id = str(uuid.uuid4())  # Generate a unique ID for the chunk\n\
          \                content_token_count = chunker.tokenizer.count_tokens(raw_chunk)\n\
          \n                # Prepare metadata object\n                metadata_obj\
          \ = {\n                    \"file_name\": file_name,\n                 \
          \   \"document_id\": chunk_id,\n                    \"token_count\": content_token_count,\n\
          \                }\n\n                metadata_str = json.dumps(metadata_obj)\n\
          \                metadata_token_count = chunker.tokenizer.count_tokens(metadata_str)\n\
          \                metadata_obj[\"metadata_token_count\"] = metadata_token_count\n\
          \n                chunks_with_embedding.append(\n                    {\n\
          \                        \"content\": raw_chunk,\n                     \
          \   \"mime_type\": \"text/markdown\",\n                        \"embedding\"\
          : embedding,\n                        \"metadata\": metadata_obj,\n    \
          \                }\n                )\n            if chunks_with_embedding:\n\
          \                try:\n                    client.vector_io.insert(\n  \
          \                      vector_db_id=vector_db_id, chunks=chunks_with_embedding\n\
          \                    )\n                except Exception as e:\n       \
          \             _log.error(f\"Failed to insert embeddings into vector database:\
          \ {e}\")\n\n        _log.info(f\"Processed {processed_docs} documents successfully.\"\
          )\n\n    # ---- Main logic ----\n    input_path = pathlib.Path(input_path)\n\
          \    output_path = pathlib.Path(output_path)\n    output_path.mkdir(parents=True,\
          \ exist_ok=True)\n\n    # Original code using splits\n    input_pdfs = [input_path\
          \ / name for name in pdf_split]\n    # Alternative not using splits\n  \
          \  # input_pdfs = pathlib.Path(input_path).glob(\"*.pdf\")\n\n    # Required\
          \ models are automatically downloaded when they are\n    # not provided\
          \ in PdfPipelineOptions initialization\n    pipeline_options = PdfPipelineOptions()\n\
          \    pipeline_options.do_ocr = True\n    pipeline_options.generate_page_images\
          \ = True\n    pipeline_options.ocr_options = RapidOcrOptions()\n\n    doc_converter\
          \ = DocumentConverter(\n        format_options={\n            InputFormat.PDF:\
          \ PdfFormatOption(pipeline_options=pipeline_options)\n        }\n    )\n\
          \n    conv_results = doc_converter.convert_all(\n        input_pdfs,\n \
          \       raises_on_error=True,\n    )\n\n    # Initialize LlamaStack client\n\
          \    client = LlamaStackClient(base_url=service_url)\n\n    # Process the\
          \ conversion results and insert embeddings into the vector database\n  \
          \  process_and_insert_embeddings(conv_results)\n\n"
        image: quay.io/modh/odh-pipeline-runtime-pytorch-cuda-py311-ubi9@sha256:4706be608af3f33c88700ef6ef6a99e716fc95fc7d2e879502e81c0022fd840e
        resources:
          cpuLimit: 4.0
          cpuRequest: 0.5
          memoryLimit: 6.442450944
          memoryRequest: 2.147483648
          resourceCpuLimit: '4'
          resourceCpuRequest: 500m
          resourceMemoryLimit: 6Gi
          resourceMemoryRequest: 2Gi
    exec-import-test-pdfs:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - import_test_pdfs
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'requests' &&\
          \ \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef import_test_pdfs(\n    base_url: str,\n    pdf_filenames: str,\n\
          \    output_path: dsl.OutputPath(\"input-pdfs\"),\n):\n    import os\n \
          \   import requests\n    import shutil\n\n    os.makedirs(output_path, exist_ok=True)\n\
          \    filenames = [f.strip() for f in pdf_filenames.split(\",\") if f.strip()]\n\
          \n    for filename in filenames:\n        url = f\"{base_url.rstrip('/')}/{filename}\"\
          \n        file_path = os.path.join(output_path, filename)\n\n        try:\n\
          \            with requests.get(url, stream=True, timeout=10) as response:\n\
          \                response.raise_for_status()\n                with open(file_path,\
          \ \"wb\") as f:\n                    shutil.copyfileobj(response.raw, f)\n\
          \            print(f\"Downloaded {filename}\")\n        except requests.exceptions.RequestException\
          \ as e:\n            print(f\"Failed to download {filename}: {e}, skipping.\"\
          )\n\n"
        image: registry.redhat.io/ubi9/python-312@sha256:e80ff3673c95b91f0dafdbe97afb261eab8244d7fd8b47e20ffcbcfee27fb168
    exec-register-vector-db:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - register_vector_db
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'llama-stack-client'\
          \ 'fire' 'requests' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef register_vector_db(\n    service_url: str,\n    vector_db_id:\
          \ str,\n    embed_model_id: str,\n):\n    from llama_stack_client import\
          \ LlamaStackClient\n\n    client = LlamaStackClient(base_url=service_url)\n\
          \n    models = client.models.list()\n    matching_model = next(\n      \
          \  (m for m in models if m.provider_resource_id == embed_model_id), None\n\
          \    )\n\n    if not matching_model:\n        raise ValueError(\n      \
          \      f\"Model with ID '{embed_model_id}' not found on LlamaStack server.\"\
          \n        )\n\n    if matching_model.model_type != \"embedding\":\n    \
          \    raise ValueError(f\"Model '{embed_model_id}' is not an embedding model\"\
          )\n\n    embedding_dimension = matching_model.metadata[\"embedding_dimension\"\
          ]\n\n    # Register the vector DB\n    _ = client.vector_dbs.register(\n\
          \        vector_db_id=vector_db_id,\n        embedding_model=matching_model.identifier,\n\
          \        embedding_dimension=embedding_dimension,\n        provider_id=\"\
          milvus\",\n    )\n    print(\n        f\"Registered vector DB '{vector_db_id}'\
          \ with embedding model '{embed_model_id}'.\"\n    )\n\n"
        image: registry.redhat.io/ubi9/python-312@sha256:e80ff3673c95b91f0dafdbe97afb261eab8244d7fd8b47e20ffcbcfee27fb168
pipelineInfo:
  description: Converts PDF documents in a git repository to Markdown using Docling
    and generates embeddings
  name: docling-convert-pipeline
root:
  dag:
    tasks:
      create-pdf-splits:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-create-pdf-splits
        dependentTasks:
        - import-test-pdfs
        inputs:
          artifacts:
            input_path:
              taskOutputArtifact:
                outputArtifactKey: output_path
                producerTask: import-test-pdfs
          parameters:
            num_splits:
              componentInputParameter: num_workers
        taskInfo:
          name: create-pdf-splits
      for-loop-1:
        componentRef:
          name: comp-for-loop-1
        dependentTasks:
        - create-pdf-splits
        - import-test-pdfs
        inputs:
          artifacts:
            pipelinechannel--import-test-pdfs-output_path:
              taskOutputArtifact:
                outputArtifactKey: output_path
                producerTask: import-test-pdfs
          parameters:
            pipelinechannel--create-pdf-splits-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: create-pdf-splits
            pipelinechannel--embed_model_id:
              componentInputParameter: embed_model_id
            pipelinechannel--max_tokens:
              componentInputParameter: max_tokens
            pipelinechannel--service_url:
              componentInputParameter: service_url
            pipelinechannel--use_gpu:
              componentInputParameter: use_gpu
            pipelinechannel--vector_db_id:
              componentInputParameter: vector_db_id
        parameterIterator:
          itemInput: pipelinechannel--create-pdf-splits-Output-loop-item
          items:
            inputParameter: pipelinechannel--create-pdf-splits-Output
        taskInfo:
          name: for-loop-1
      import-test-pdfs:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-import-test-pdfs
        inputs:
          parameters:
            base_url:
              componentInputParameter: base_url
            pdf_filenames:
              componentInputParameter: pdf_filenames
        taskInfo:
          name: import-test-pdfs
      register-vector-db:
        cachingOptions: {}
        componentRef:
          name: comp-register-vector-db
        inputs:
          parameters:
            embed_model_id:
              componentInputParameter: embed_model_id
            service_url:
              componentInputParameter: service_url
            vector_db_id:
              componentInputParameter: vector_db_id
        taskInfo:
          name: register-vector-db
  inputDefinitions:
    parameters:
      base_url:
        defaultValue: https://raw.githubusercontent.com/docling-project/docling/main/tests/data/pdf
        description: Base URL to fetch PDF files from
        isOptional: true
        parameterType: STRING
      embed_model_id:
        defaultValue: ibm-granite/granite-embedding-125m-english
        description: Model ID for embedding generation
        isOptional: true
        parameterType: STRING
      max_tokens:
        defaultValue: 512.0
        description: Maximum number of tokens per chunk
        isOptional: true
        parameterType: NUMBER_INTEGER
      num_workers:
        defaultValue: 1.0
        description: Number of docling worker pods to use
        isOptional: true
        parameterType: NUMBER_INTEGER
      pdf_filenames:
        defaultValue: 2203.01017v2.pdf, 2206.01062.pdf, 2305.03393v1-pg9.pdf, amt_handbook_sample.pdf,
          code_and_formula.pdf, multi_page.pdf, picture_classification.pdf, redp5110_sampled.pdf,
          right_to_left_01.pdf, right_to_left_02.pdf, right_to_left_03.pdf
        description: Comma-separated list of PDF filenames to download and convert
        isOptional: true
        parameterType: STRING
      service_url:
        defaultValue: http://lsd-llama-milvus-service:8321
        description: URL of the Milvus service
        isOptional: true
        parameterType: STRING
      use_gpu:
        defaultValue: true
        description: Enable GPU in the docling workers
        isOptional: true
        parameterType: BOOLEAN
      vector_db_id:
        defaultValue: my_demo_vector_id
        description: ID of the vector database to store embeddings
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.13.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-docling-convert:
          nodeSelector:
            nodeSelectorJson:
              runtimeValue:
                constant: {}
          tolerations:
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
