{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install the llama stack client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(identifier='vllm', metadata={}, api_model_type='llm', provider_id='vllm-inference', type='model', provider_resource_id='vllm', model_type='llm'), Model(identifier='ibm-granite/granite-embedding-125m-english', metadata={'embedding_dimension': 768.0}, api_model_type='embedding', provider_id='sentence-transformers', type='model', provider_resource_id='ibm-granite/granite-embedding-125m-english', model_type='embedding')]\n",
      "=== Available Vector Databases ===\n",
      "- ID: my_demo_image_ocr_vector_id\n",
      "  Provider: milvus\n",
      "  Embedding Model: ibm-granite/granite-embedding-125m-english\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "client = LlamaStackClient(base_url=\"http://lsd-llama-milvus-service:8321\")\n",
    "print(client.models.list())\n",
    "\n",
    "# Check what vector databases exist\n",
    "print(\"=== Available Vector Databases ===\")\n",
    "vector_dbs = client.vector_dbs.list()\n",
    "if vector_dbs:\n",
    "    for vdb in vector_dbs:\n",
    "        print(f\"- ID: {vdb.identifier}\")\n",
    "        print(f\"  Provider: {vdb.provider_id}\")\n",
    "        print(f\"  Embedding Model: {vdb.embedding_model}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No vector databases found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import and run the KubeFlow Pipeline\n",
    "Import the \"[docling_convert_images_pipeline_ocr_only_compiled.yaml](./docling_convert_images_pipeline_ocr_only_compiled.yaml)\" KubeFlow Pipeline into your pipeline server, then run the pipeline to insert your PDF documents into the vector database.\n",
    "\n",
    "When running the pipeline, you can customize the following parameters:\n",
    "\n",
    "- `base_url`: Base URL to fetch Image files from\n",
    "- `image_filenames`: Comma-separated list of PNG/JPG filenames to download and convert\n",
    "- `num_workers`: Number of parallel workers\n",
    "- `vector_db_id`: Milvus vector database ID\n",
    "- `service_url`: Milvus service URL\n",
    "- `embed_model_id`: Embedding model to use\n",
    "- `max_tokens`: Maximum tokens per chunk\n",
    "- `use_gpu`: Enable/disable GPU acceleration\n",
    "\n",
    "Note: The compiled pipeline was generated by running `python docling_convert_images_pipeline_ocr_only.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt the LLM\n",
    "Prompt the LLM with a question in relation to the documents inserted, and see it return accurate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/tools?toolgroup_id=builtin%3A%3Arag%2Fknowledge_search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/55314250-e41e-4fa3-896c-41262dc57472/session \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/55314250-e41e-4fa3-896c-41262dc57472/session/f415ad31-e62e-4698-8de4-1d807225841a/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> List RAG key market use cases\n",
      "\u001b[33minference> \u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'RAG key market use cases'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 2 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\nKnowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\nCode Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\nRecommendation Systems: Enhancing recommendations by providing relevant context.\\nCustomer Service: Improving support accuracy with access to current product information.\\nPersonal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\nMulti-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\nLegal Applications: Retrieving legal documents and case law for reliable legal opinions.\\nGeneral Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is recommendations .\\nMetadata: {'file_name': 'key-market-usecases', 'document_id': '25a03b33-0bb8-4e78-bbf2-746445152cb1'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: Ingeslion Flow\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\n(Chal VI)\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\n(LS ClienL)\\nQuery Select Wodel\\nMetadata: {'file_name': 'diagram', 'document_id': '231c0859-e0ef-42d2-9ec4-9e8810822a7f'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"RAG key market use cases\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m key\u001b[0m\u001b[33m market\u001b[0m\u001b[33m use\u001b[0m\u001b[33m cases\u001b[0m\u001b[33m for\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRe\u001b[0m\u001b[33mactive\u001b[0m\u001b[33m Application\u001b[0m\u001b[33m Generator\u001b[0m\u001b[33m)\u001b[0m\u001b[33m are\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m Question\u001b[0m\u001b[33m Answer\u001b[0m\u001b[33ming\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Providing\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m answers\u001b[0m\u001b[33m in\u001b[0m\u001b[33m customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m,\u001b[0m\u001b[33m product\u001b[0m\u001b[33m manuals\u001b[0m\u001b[33m,\u001b[0m\u001b[33m or\u001b[0m\u001b[33m FAQs\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Code\u001b[0m\u001b[33m Generation\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mving\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m code\u001b[0m\u001b[33m snippets\u001b[0m\u001b[33m and\u001b[0m\u001b[33m documentation\u001b[0m\u001b[33m to\u001b[0m\u001b[33m assist\u001b[0m\u001b[33m in\u001b[0m\u001b[33m code\u001b[0m\u001b[33m creation\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Recommendation\u001b[0m\u001b[33m Systems\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Enh\u001b[0m\u001b[33mancing\u001b[0m\u001b[33m recommendations\u001b[0m\u001b[33m by\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m context\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Customer\u001b[0m\u001b[33m Service\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Impro\u001b[0m\u001b[33mving\u001b[0m\u001b[33m support\u001b[0m\u001b[33m accuracy\u001b[0m\u001b[33m with\u001b[0m\u001b[33m access\u001b[0m\u001b[33m to\u001b[0m\u001b[33m current\u001b[0m\u001b[33m product\u001b[0m\u001b[33m information\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Personal\u001b[0m\u001b[33m Assist\u001b[0m\u001b[33mants\u001b[0m\u001b[33m:\u001b[0m\u001b[33m En\u001b[0m\u001b[33mabling\u001b[0m\u001b[33m more\u001b[0m\u001b[33m comprehensive\u001b[0m\u001b[33m and\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m information\u001b[0m\u001b[33m from\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m assistants\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m6\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Multi\u001b[0m\u001b[33m-hop\u001b[0m\u001b[33m Question\u001b[0m\u001b[33m Answer\u001b[0m\u001b[33ming\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Handling\u001b[0m\u001b[33m complex\u001b[0m\u001b[33m,\u001b[0m\u001b[33m multi\u001b[0m\u001b[33m-step\u001b[0m\u001b[33m questions\u001b[0m\u001b[33m through\u001b[0m\u001b[33m iterative\u001b[0m\u001b[33m retrieval\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m7\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Legal\u001b[0m\u001b[33m Applications\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mving\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m and\u001b[0m\u001b[33m case\u001b[0m\u001b[33m law\u001b[0m\u001b[33m for\u001b[0m\u001b[33m reliable\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m opinions\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m8\u001b[0m\u001b[33m.\u001b[0m\u001b[33m General\u001b[0m\u001b[33m Task\u001b[0m\u001b[33m Assistance\u001b[0m\u001b[33m:\u001b[0m\u001b[33m A\u001b[0m\u001b[33miding\u001b[0m\u001b[33m users\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m requiring\u001b[0m\u001b[33m information\u001b[0m\u001b[33m access\u001b[0m\u001b[33m and\u001b[0m\u001b[33m decision\u001b[0m\u001b[33m-making\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThese\u001b[0m\u001b[33m use\u001b[0m\u001b[33m cases\u001b[0m\u001b[33m demonstrate\u001b[0m\u001b[33m the\u001b[0m\u001b[33m versatility\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m industries\u001b[0m\u001b[33m and\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m,\u001b[0m\u001b[33m highlighting\u001b[0m\u001b[33m its\u001b[0m\u001b[33m potential\u001b[0m\u001b[33m to\u001b[0m\u001b[33m improve\u001b[0m\u001b[33m efficiency\u001b[0m\u001b[33m,\u001b[0m\u001b[33m accuracy\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m user\u001b[0m\u001b[33m experience\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "import uuid\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=\"vllm\",\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\"vector_db_ids\": [\"my_demo_image_ocr_vector_id\"]},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt = \"List RAG key market use cases\"\n",
    "print(\"prompt>\", prompt)\n",
    "\n",
    "session_id = rag_agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n",
    "\n",
    "response = rag_agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    session_id=session_id,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for log in AgentEventLogger().log(response):\n",
    "    log.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/55314250-e41e-4fa3-896c-41262dc57472/session/f415ad31-e62e-4698-8de4-1d807225841a/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Describe the sequence of steps of the Ingestion Flow\n",
      "\u001b[33minference> \u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'Ingestion Flow RAG sequence of steps'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 2 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: Market Use Cases Key\\nRAG is being adopted across various industries for diverse applications; including:\\nKnowledge Question Answering: Providing accurate answers in customer service product manuals or FAQs. using\\nCode Generation: Retrieving relevant code snippets and documentation to assist in code creation.\\nRecommendation Systems: Enhancing recommendations by providing relevant context.\\nCustomer Service: Improving support accuracy with access to current product information.\\nPersonal Assistants: Enabling more comprehensive and accurate information from Al assistants .\\nMulti-hop Question Answering: Handling complex; multi-step questions through iterative retrieval.\\nLegal Applications: Retrieving legal documents and case law for reliable legal opinions.\\nGeneral Task Assistance: Aiding users in various tasks requiring information access and decision-making:\\nThe rising demand for hyper-personalized content in areas like marketing and e-commerce is recommendations .\\nMetadata: {'file_name': 'key-market-usecases', 'document_id': '25a03b33-0bb8-4e78-bbf2-746445152cb1'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: Ingeslion Flow\\nDocling (Chunking\\nEmbedding (Granite\\nEmbedding via vLLM)\\n(Yector DB}\\nTop-K Chunks\\nPrompt Constructor\\n(llama-stack)\\nUser Inlerface\\n(Chal VI)\\nQuery\\nQuery Embedding\\nLLM Inference\\nResfonse\\nJupyter Notebooks\\n(Run\\nDebug)\\nQuery\\nClient\\n(LS ClienL)\\nQuery Select Wodel\\nMetadata: {'file_name': 'diagram', 'document_id': '231c0859-e0ef-42d2-9ec4-9e8810822a7f'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"Ingestion Flow RAG sequence of steps\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33mBased\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m sequence\u001b[0m\u001b[33m of\u001b[0m\u001b[33m steps\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m In\u001b[0m\u001b[33mgest\u001b[0m\u001b[33mion\u001b[0m\u001b[33m Flow\u001b[0m\u001b[33m for\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m is\u001b[0m\u001b[33m as\u001b[0m\u001b[33m follows\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mDoc\u001b[0m\u001b[33mling\u001b[0m\u001b[33m (\u001b[0m\u001b[33mChunk\u001b[0m\u001b[33ming\u001b[0m\u001b[33m)**\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Breaking\u001b[0m\u001b[33m down\u001b[0m\u001b[33m the\u001b[0m\u001b[33m input\u001b[0m\u001b[33m data\u001b[0m\u001b[33m into\u001b[0m\u001b[33m smaller\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m or\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mEmbed\u001b[0m\u001b[33mding\u001b[0m\u001b[33m (\u001b[0m\u001b[33mGran\u001b[0m\u001b[33mite\u001b[0m\u001b[33m)**\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Con\u001b[0m\u001b[33mverting\u001b[0m\u001b[33m the\u001b[0m\u001b[33m chunk\u001b[0m\u001b[33med\u001b[0m\u001b[33m data\u001b[0m\u001b[33m into\u001b[0m\u001b[33m numerical\u001b[0m\u001b[33m representations\u001b[0m\u001b[33m using\u001b[0m\u001b[33m embedding\u001b[0m\u001b[33m techniques\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mEmbed\u001b[0m\u001b[33mding\u001b[0m\u001b[33m via\u001b[0m\u001b[33m v\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mM\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Using\u001b[0m\u001b[33m a\u001b[0m\u001b[33m large\u001b[0m\u001b[33m language\u001b[0m\u001b[33m model\u001b[0m\u001b[33m (\u001b[0m\u001b[33mv\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mM\u001b[0m\u001b[33m)\u001b[0m\u001b[33m to\u001b[0m\u001b[33m further\u001b[0m\u001b[33m refine\u001b[0m\u001b[33m the\u001b[0m\u001b[33m embeddings\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mY\u001b[0m\u001b[33mector\u001b[0m\u001b[33m DB\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m St\u001b[0m\u001b[33moring\u001b[0m\u001b[33m the\u001b[0m\u001b[33m embedded\u001b[0m\u001b[33m data\u001b[0m\u001b[33m in\u001b[0m\u001b[33m a\u001b[0m\u001b[33m database\u001b[0m\u001b[33m for\u001b[0m\u001b[33m later\u001b[0m\u001b[33m use\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mTop\u001b[0m\u001b[33m-K\u001b[0m\u001b[33m Ch\u001b[0m\u001b[33munks\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Select\u001b[0m\u001b[33ming\u001b[0m\u001b[33m the\u001b[0m\u001b[33m top\u001b[0m\u001b[33m-k\u001b[0m\u001b[33m most\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m based\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m embedded\u001b[0m\u001b[33m data\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m6\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mPrompt\u001b[0m\u001b[33m Constructor\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Construct\u001b[0m\u001b[33ming\u001b[0m\u001b[33m a\u001b[0m\u001b[33m prompt\u001b[0m\u001b[33m or\u001b[0m\u001b[33m query\u001b[0m\u001b[33m based\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m selected\u001b[0m\u001b[33m chunks\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m7\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mama\u001b[0m\u001b[33m-stack\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Using\u001b[0m\u001b[33m a\u001b[0m\u001b[33m large\u001b[0m\u001b[33m language\u001b[0m\u001b[33m model\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mama\u001b[0m\u001b[33m)\u001b[0m\u001b[33m to\u001b[0m\u001b[33m generate\u001b[0m\u001b[33m a\u001b[0m\u001b[33m response\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m prompt\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m8\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mUser\u001b[0m\u001b[33m Interface\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Present\u001b[0m\u001b[33ming\u001b[0m\u001b[33m the\u001b[0m\u001b[33m response\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m user\u001b[0m\u001b[33m through\u001b[0m\u001b[33m a\u001b[0m\u001b[33m user\u001b[0m\u001b[33m interface\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m9\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mQuery\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Re\u001b[0m\u001b[33mceiving\u001b[0m\u001b[33m the\u001b[0m\u001b[33m user\u001b[0m\u001b[33m's\u001b[0m\u001b[33m query\u001b[0m\u001b[33m or\u001b[0m\u001b[33m input\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m10\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mQuery\u001b[0m\u001b[33m Embed\u001b[0m\u001b[33mding\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Embed\u001b[0m\u001b[33mding\u001b[0m\u001b[33m the\u001b[0m\u001b[33m user\u001b[0m\u001b[33m's\u001b[0m\u001b[33m query\u001b[0m\u001b[33m into\u001b[0m\u001b[33m a\u001b[0m\u001b[33m numerical\u001b[0m\u001b[33m representation\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m11\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mM\u001b[0m\u001b[33m In\u001b[0m\u001b[33mference\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Using\u001b[0m\u001b[33m the\u001b[0m\u001b[33m large\u001b[0m\u001b[33m language\u001b[0m\u001b[33m model\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLL\u001b[0m\u001b[33mama\u001b[0m\u001b[33m)\u001b[0m\u001b[33m to\u001b[0m\u001b[33m generate\u001b[0m\u001b[33m a\u001b[0m\u001b[33m response\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m user\u001b[0m\u001b[33m's\u001b[0m\u001b[33m query\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m12\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mResponse\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Generating\u001b[0m\u001b[33m a\u001b[0m\u001b[33m response\u001b[0m\u001b[33m to\u001b[0m\u001b[33m the\u001b[0m\u001b[33m user\u001b[0m\u001b[33m's\u001b[0m\u001b[33m query\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m13\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mJ\u001b[0m\u001b[33mupyter\u001b[0m\u001b[33m Note\u001b[0m\u001b[33mbooks\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Running\u001b[0m\u001b[33m and\u001b[0m\u001b[33m debugging\u001b[0m\u001b[33m the\u001b[0m\u001b[33m response\u001b[0m\u001b[33m in\u001b[0m\u001b[33m J\u001b[0m\u001b[33mupyter\u001b[0m\u001b[33m Note\u001b[0m\u001b[33mbooks\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m14\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mQuery\u001b[0m\u001b[33m Client\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Sending\u001b[0m\u001b[33m the\u001b[0m\u001b[33m query\u001b[0m\u001b[33m to\u001b[0m\u001b[33m a\u001b[0m\u001b[33m query\u001b[0m\u001b[33m client\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m15\u001b[0m\u001b[33m.\u001b[0m\u001b[33m **\u001b[0m\u001b[33mQuery\u001b[0m\u001b[33m Select\u001b[0m\u001b[33m W\u001b[0m\u001b[33model\u001b[0m\u001b[33m**:\u001b[0m\u001b[33m Select\u001b[0m\u001b[33ming\u001b[0m\u001b[33m the\u001b[0m\u001b[33m best\u001b[0m\u001b[33m response\u001b[0m\u001b[33m from\u001b[0m\u001b[33m a\u001b[0m\u001b[33m set\u001b[0m\u001b[33m of\u001b[0m\u001b[33m responses\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThis\u001b[0m\u001b[33m sequence\u001b[0m\u001b[33m of\u001b[0m\u001b[33m steps\u001b[0m\u001b[33m outlines\u001b[0m\u001b[33m the\u001b[0m\u001b[33m process\u001b[0m\u001b[33m of\u001b[0m\u001b[33m ing\u001b[0m\u001b[33mesting\u001b[0m\u001b[33m data\u001b[0m\u001b[33m,\u001b[0m\u001b[33m processing\u001b[0m\u001b[33m it\u001b[0m\u001b[33m through\u001b[0m\u001b[33m various\u001b[0m\u001b[33m techniques\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m generating\u001b[0m\u001b[33m a\u001b[0m\u001b[33m response\u001b[0m\u001b[33m to\u001b[0m\u001b[33m user\u001b[0m\u001b[33m queries\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "prompt = \"Describe the sequence of steps of the Ingestion Flow\"\n",
    "print(\"prompt>\", prompt)\n",
    "\n",
    "# session_id = rag_agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n",
    "\n",
    "response = rag_agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    session_id=session_id,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for log in AgentEventLogger().log(response):\n",
    "    log.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or Query chunks from a vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = client.vector_io.query(\n",
    "    vector_db_id=vector_db_id,\n",
    "    query=\"what do you know about?\",\n",
    ")\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You've successfully inserted your PDF documents via a KubeFlow Pipeline, and queried your RAG application using Llama Stack! ðŸŽ‰ðŸ¥³"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
