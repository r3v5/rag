{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Llama Stack client, list available models and vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models information: [Model(identifier='vllm', metadata={}, api_model_type='llm', provider_id='vllm-inference', type='model', provider_resource_id='vllm', model_type='llm'), Model(identifier='granite-embedding-125m', metadata={'embedding_dimension': 768.0}, api_model_type='embedding', provider_id='sentence-transformers', type='model', provider_resource_id='ibm-granite/granite-embedding-125m-english', model_type='embedding')]\n",
      "\n",
      "Identifier for Inference model in usage: vllm\n",
      "\n",
      "=== Available Vector Databases ===\n",
      "- ID: asr-vector-db\n",
      "  Provider: milvus\n",
      "  Embedding Model: granite-embedding-125m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(base_url=\"http://lsd-llama-milvus-service:8321\")\n",
    "\n",
    "models = client.models.list()\n",
    "print(f\"Models information: {models}\\n\")\n",
    "\n",
    "inference_llm = next(\n",
    "    (model.identifier for model in models if model.model_type == \"llm\"), None\n",
    ")\n",
    "print(f\"Identifier for Inference model in usage: {inference_llm}\\n\")\n",
    "\n",
    "# Check what vector databases exist\n",
    "print(\"=== Available Vector Databases ===\")\n",
    "vector_dbs = client.vector_dbs.list()\n",
    "if vector_dbs:\n",
    "    for vdb in vector_dbs:\n",
    "        print(f\"- ID: {vdb.identifier}\")\n",
    "        print(f\"  Provider: {vdb.provider_id}\")\n",
    "        print(f\"  Embedding Model: {vdb.embedding_model}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No vector databases found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create RAG Agent and prompt the LLM\n",
    "Prompt the LLM with questions in relation to the documents inserted, and see it return accurate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/tools?toolgroup_id=builtin%3A%3Arag%2Fknowledge_search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/f79e548b-4078-4d37-bdb6-e82a8d190dee/session \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/f79e548b-4078-4d37-bdb6-e82a8d190dee/session/6b9db4ef-121c-46fc-bbe5-bf4772c9aac1/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> List RAG key market use cases\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'RAG key market use cases'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: Key market use cases. RAC is being adopted across various industries for diverse applications,\\nincluding knowledge question answering, providing accurate answers in customer service using product\\nmanuals or fax. Code generation, retrieving relevant code snippets and documentation to\\nassist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\ncontext. Customer service, improving support accuracy with access to current product information.\\nPersonal assistance, enabling more comprehensive and accurate information from AI assistants.\\nMulti-hub question answering, handling complex multi-step questions through iterative retrieval.\\nLegal applications, retrieving legal documents and case law for reliable legal opinions.\\nGeneral task assistance, aiding users in various tasks requiring information access and decision\\nmaking. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\nis also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\nPopular patience, promoted經 Triологi yokov sources of resources that have tested for\\nimportant יה limit. Online,\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '69a12f52-41d0-4fe2-a7f8-958ca48e4969'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: The benefits of RAC\\nThe retrieval mechanisms built into a RAC architecture allow it to tap into additional data sources beyond an LLM general training.\\nGrounding an LLM on a set of external verifiable facts via RAC supports several beneficial goals.\\nAccuracy\\nRAC provides an LLM with sources it can cite so users can verify these claims.\\nYou can also design a RAC architecture to respond with I don't know if the question is outside the scope of its knowledge.\\nOverall RAC reduces the chances of an LLM sharing incorrect or misleading information as an output and may increase user trust.\\nCost effectiveness\\nRetraining and fine-tuning LLMs costly and time-consuming as it's creating a foundation model to build something like a chatbot from scratch with domain-specific information.\\nWith RAC a user can introduce new data to an LLM as well as swap out or update sources of information by simply uploading a document or file.\\nRAC can also reduce inference codes.\\nLLM queries are expensive, placing demands on your own hardware if you run a local model or running up a mattered bill if you use an external service through an application programming interface.\\nRather than sending an entire reference document to an LLM at once, RAC can send only the most relevant chunks of the reference material, thereby reducing the size of queries and improving efficiency.\\nDeveloper Control\\nCompared to traditional fine-tuning methods, RAC provides a more accessible and straightforward way to get feedback, troubleshoot and fix applications.\\nFor developers, the biggest benefit of RAC architecture is that it lets them take advantage of a stream of domain-specific and up-to-date information.\\nData sovereignty and privacy\\nUsing confidential information to fine-tune an LLM tool has historically been risky as LLMs can reveal information from their training data.\\nRAC offers a solution to these privacy concerns by allowing sensitive data to remain on-premise while still being used to inform a local LLM or a trusted external LLM.\\nRAC architecture can also be set up to restrict sensitive information retrieval to different authorization levels.\\nThat is, certain users can access certain information based on their security clearance levels.\\nThese values can be faced from president to the right of this position where the field you have been claiming that breachesayd 상 and pressing follow certain modules.\\nMetadata: {'file_name': 'tmp51upy7_e_RAG_benefits', 'document_id': 'f928ee9b-f644-4125-8c43-83d9da528dfe'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: RAC vs. Regular LLM Outputs\\nLLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\nAI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\nLLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\nLLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\nLLMs have a knowledge cut-off date, meaning the information they've been trained on doesn't continuously gather updates.\\nAs a result, the resource material can become outdated and no longer relevant.\\nLLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\nImplementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\nThe LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\nLLM and our continental\\nMuch of communication\\nLLMs 트랁\\nMetadata: {'file_name': 'tmpwloo59c6_RAG_vs_Regular_LLM_Output', 'document_id': 'b2b26438-2fe8-47fb-a77c-4360c4be4320'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: Clarifying target audience and user roles. This document clarifies the target audience and user\\nroles for Red Hat Rack project focusing on the distinction between end users and builders.\\nEnd users vs. builders. End users consume the final product.\\nInteract with a chat GPT-like application. Builders create and configure the AI systems\\nused by end users. Configure a Rack backend tweaking parameters for a specific\\nexperience such as chat GPT. We are targeting builders not end users. Builders optimize\\ntheir systems for their specific end users. Builder archetypes. High-coder builders aka\\nProCode prefer SDKs and code-based solutions. They need access to all configurable\\nparameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\nsystem via a UI. Example, LamaStackCliMyRackApp.py-web.\\nLow-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\nsystems. They could benefit from tools like the existing LamaStackPlayground and so on.\\nBuilders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\nand its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\nwhether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.\\nPlatformers focus on building, maintaining, and securing the AI platform and APIs. They serve\\nboth builders for development and opsors for development slash operations, ensuring infrastructure\\nis reliable, scalable, and supports self-service.\\nOpsers. AI ML Ops Engineers. Opsers focus on operationalizing and automating the AI ML life\\ncycle. For example, they use platform APIs to deploy, monitor, and manage models, enabling\\nbuilders models to reach and succeed in production. Opsors work closely with platformers to ensure\\ninfrastructure meets operational needs.\\nIn summary, platformers enable builders and other developers to create systems for end-users.\\nThe Red Hat Rack team focus is on empowering builders with the tools and flexibility they need to\\nbuild the best experience for their end-users.\\nThere are new levers there are new leaders.\\nManyоф methods of lobbying.\\nThere are streams that have owned by AWS company its venir using a light El��게 and construction\\nprotection.\\nMetadata: {'file_name': 'RAG_customers', 'document_id': 'd33413d1-5770-42ab-9851-e4e6c0cb661a'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: However, they can also be found on the homepage in journalism category from the provincial levels, starting the nationalvik function\\nat the GZ is a service where we continue andём back in regulators where you are sacrificing data now.\\nMetadata: {'file_name': 'tmp51upy7_e_RAG_benefits', 'document_id': '4d90e27d-5c78-42a0-a84f-28bd727b0b63'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"RAG key market use cases\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mR\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRet\u001b[0m\u001b[33mrie\u001b[0m\u001b[33mval\u001b[0m\u001b[33m-Aug\u001b[0m\u001b[33mmented\u001b[0m\u001b[33m Generation\u001b[0m\u001b[33m)\u001b[0m\u001b[33m key\u001b[0m\u001b[33m market\u001b[0m\u001b[33m use\u001b[0m\u001b[33m cases\u001b[0m\u001b[33m include\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m question\u001b[0m\u001b[33m answering\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Providing\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m answers\u001b[0m\u001b[33m in\u001b[0m\u001b[33m customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m using\u001b[0m\u001b[33m product\u001b[0m\u001b[33m manuals\u001b[0m\u001b[33m or\u001b[0m\u001b[33m fax\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Code\u001b[0m\u001b[33m generation\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mving\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m code\u001b[0m\u001b[33m snippets\u001b[0m\u001b[33m and\u001b[0m\u001b[33m documentation\u001b[0m\u001b[33m to\u001b[0m\u001b[33m assist\u001b[0m\u001b[33m in\u001b[0m\u001b[33m code\u001b[0m\u001b[33m creation\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Recommendation\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Enh\u001b[0m\u001b[33mancing\u001b[0m\u001b[33m recommendations\u001b[0m\u001b[33m by\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m context\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Impro\u001b[0m\u001b[33mving\u001b[0m\u001b[33m support\u001b[0m\u001b[33m accuracy\u001b[0m\u001b[33m with\u001b[0m\u001b[33m access\u001b[0m\u001b[33m to\u001b[0m\u001b[33m current\u001b[0m\u001b[33m product\u001b[0m\u001b[33m information\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Personal\u001b[0m\u001b[33m assistance\u001b[0m\u001b[33m:\u001b[0m\u001b[33m En\u001b[0m\u001b[33mabling\u001b[0m\u001b[33m more\u001b[0m\u001b[33m comprehensive\u001b[0m\u001b[33m and\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m information\u001b[0m\u001b[33m from\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m assistants\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m6\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Multi\u001b[0m\u001b[33m-h\u001b[0m\u001b[33mub\u001b[0m\u001b[33m question\u001b[0m\u001b[33m answering\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Handling\u001b[0m\u001b[33m complex\u001b[0m\u001b[33m multi\u001b[0m\u001b[33m-step\u001b[0m\u001b[33m questions\u001b[0m\u001b[33m through\u001b[0m\u001b[33m iterative\u001b[0m\u001b[33m retrieval\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m7\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Legal\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Retrie\u001b[0m\u001b[33mving\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m documents\u001b[0m\u001b[33m and\u001b[0m\u001b[33m case\u001b[0m\u001b[33m law\u001b[0m\u001b[33m for\u001b[0m\u001b[33m reliable\u001b[0m\u001b[33m legal\u001b[0m\u001b[33m opinions\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m8\u001b[0m\u001b[33m.\u001b[0m\u001b[33m General\u001b[0m\u001b[33m task\u001b[0m\u001b[33m assistance\u001b[0m\u001b[33m:\u001b[0m\u001b[33m A\u001b[0m\u001b[33miding\u001b[0m\u001b[33m users\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m tasks\u001b[0m\u001b[33m requiring\u001b[0m\u001b[33m information\u001b[0m\u001b[33m access\u001b[0m\u001b[33m and\u001b[0m\u001b[33m decision\u001b[0m\u001b[33m making\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m9\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Hyper\u001b[0m\u001b[33m-person\u001b[0m\u001b[33mal\u001b[0m\u001b[33mized\u001b[0m\u001b[33m content\u001b[0m\u001b[33m:\u001b[0m\u001b[33m All\u001b[0m\u001b[33mowing\u001b[0m\u001b[33m for\u001b[0m\u001b[33m tailored\u001b[0m\u001b[33m ad\u001b[0m\u001b[33m copy\u001b[0m\u001b[33m and\u001b[0m\u001b[33m product\u001b[0m\u001b[33m recommendations\u001b[0m\u001b[33m in\u001b[0m\u001b[33m areas\u001b[0m\u001b[33m like\u001b[0m\u001b[33m marketing\u001b[0m\u001b[33m and\u001b[0m\u001b[33m e\u001b[0m\u001b[33m-commerce\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m10\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Popular\u001b[0m\u001b[33m patience\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Prom\u001b[0m\u001b[33moting\u001b[0m\u001b[33m the\u001b[0m\u001b[33m use\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m industries\u001b[0m\u001b[33m and\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThese\u001b[0m\u001b[33m use\u001b[0m\u001b[33m cases\u001b[0m\u001b[33m highlight\u001b[0m\u001b[33m the\u001b[0m\u001b[33m potential\u001b[0m\u001b[33m benefits\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m in\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m and\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m information\u001b[0m\u001b[33m to\u001b[0m\u001b[33m users\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m its\u001b[0m\u001b[33m potential\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m in\u001b[0m\u001b[33m various\u001b[0m\u001b[33m fields\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m customer\u001b[0m\u001b[33m service\u001b[0m\u001b[33m,\u001b[0m\u001b[33m code\u001b[0m\u001b[33m generation\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m recommendation\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/f79e548b-4078-4d37-bdb6-e82a8d190dee/session/6b9db4ef-121c-46fc-bbe5-bf4772c9aac1/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Name Red Hat RAG target audience and customers\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'Red Hat RAG target audience and customers'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: Clarifying target audience and user roles. This document clarifies the target audience and user\\nroles for Red Hat Rack project focusing on the distinction between end users and builders.\\nEnd users vs. builders. End users consume the final product.\\nInteract with a chat GPT-like application. Builders create and configure the AI systems\\nused by end users. Configure a Rack backend tweaking parameters for a specific\\nexperience such as chat GPT. We are targeting builders not end users. Builders optimize\\ntheir systems for their specific end users. Builder archetypes. High-coder builders aka\\nProCode prefer SDKs and code-based solutions. They need access to all configurable\\nparameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\nsystem via a UI. Example, LamaStackCliMyRackApp.py-web.\\nLow-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\nsystems. They could benefit from tools like the existing LamaStackPlayground and so on.\\nBuilders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\nand its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\nwhether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.\\nPlatformers focus on building, maintaining, and securing the AI platform and APIs. They serve\\nboth builders for development and opsors for development slash operations, ensuring infrastructure\\nis reliable, scalable, and supports self-service.\\nOpsers. AI ML Ops Engineers. Opsers focus on operationalizing and automating the AI ML life\\ncycle. For example, they use platform APIs to deploy, monitor, and manage models, enabling\\nbuilders models to reach and succeed in production. Opsors work closely with platformers to ensure\\ninfrastructure meets operational needs.\\nIn summary, platformers enable builders and other developers to create systems for end-users.\\nThe Red Hat Rack team focus is on empowering builders with the tools and flexibility they need to\\nbuild the best experience for their end-users.\\nThere are new levers there are new leaders.\\nManyоф methods of lobbying.\\nThere are streams that have owned by AWS company its venir using a light El��게 and construction\\nprotection.\\nMetadata: {'file_name': 'RAG_customers', 'document_id': 'd33413d1-5770-42ab-9851-e4e6c0cb661a'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: Key market use cases. RAC is being adopted across various industries for diverse applications,\\nincluding knowledge question answering, providing accurate answers in customer service using product\\nmanuals or fax. Code generation, retrieving relevant code snippets and documentation to\\nassist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\ncontext. Customer service, improving support accuracy with access to current product information.\\nPersonal assistance, enabling more comprehensive and accurate information from AI assistants.\\nMulti-hub question answering, handling complex multi-step questions through iterative retrieval.\\nLegal applications, retrieving legal documents and case law for reliable legal opinions.\\nGeneral task assistance, aiding users in various tasks requiring information access and decision\\nmaking. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\nis also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\nPopular patience, promoted經 Triологi yokov sources of resources that have tested for\\nimportant יה limit. Online,\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '69a12f52-41d0-4fe2-a7f8-958ca48e4969'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: The benefits of RAC\\nThe retrieval mechanisms built into a RAC architecture allow it to tap into additional data sources beyond an LLM general training.\\nGrounding an LLM on a set of external verifiable facts via RAC supports several beneficial goals.\\nAccuracy\\nRAC provides an LLM with sources it can cite so users can verify these claims.\\nYou can also design a RAC architecture to respond with I don't know if the question is outside the scope of its knowledge.\\nOverall RAC reduces the chances of an LLM sharing incorrect or misleading information as an output and may increase user trust.\\nCost effectiveness\\nRetraining and fine-tuning LLMs costly and time-consuming as it's creating a foundation model to build something like a chatbot from scratch with domain-specific information.\\nWith RAC a user can introduce new data to an LLM as well as swap out or update sources of information by simply uploading a document or file.\\nRAC can also reduce inference codes.\\nLLM queries are expensive, placing demands on your own hardware if you run a local model or running up a mattered bill if you use an external service through an application programming interface.\\nRather than sending an entire reference document to an LLM at once, RAC can send only the most relevant chunks of the reference material, thereby reducing the size of queries and improving efficiency.\\nDeveloper Control\\nCompared to traditional fine-tuning methods, RAC provides a more accessible and straightforward way to get feedback, troubleshoot and fix applications.\\nFor developers, the biggest benefit of RAC architecture is that it lets them take advantage of a stream of domain-specific and up-to-date information.\\nData sovereignty and privacy\\nUsing confidential information to fine-tune an LLM tool has historically been risky as LLMs can reveal information from their training data.\\nRAC offers a solution to these privacy concerns by allowing sensitive data to remain on-premise while still being used to inform a local LLM or a trusted external LLM.\\nRAC architecture can also be set up to restrict sensitive information retrieval to different authorization levels.\\nThat is, certain users can access certain information based on their security clearance levels.\\nThese values can be faced from president to the right of this position where the field you have been claiming that breachesayd 상 and pressing follow certain modules.\\nMetadata: {'file_name': 'tmp51upy7_e_RAG_benefits', 'document_id': 'f928ee9b-f644-4125-8c43-83d9da528dfe'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: RAC vs. Regular LLM Outputs\\nLLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\nAI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\nLLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\nLLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\nLLMs have a knowledge cut-off date, meaning the information they've been trained on doesn't continuously gather updates.\\nAs a result, the resource material can become outdated and no longer relevant.\\nLLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\nImplementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\nThe LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\nLLM and our continental\\nMuch of communication\\nLLMs 트랁\\nMetadata: {'file_name': 'tmpwloo59c6_RAG_vs_Regular_LLM_Output', 'document_id': 'b2b26438-2fe8-47fb-a77c-4360c4be4320'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: However, they can also be found on the homepage in journalism category from the provincial levels, starting the nationalvik function\\nat the GZ is a service where we continue andём back in regulators where you are sacrificing data now.\\nMetadata: {'file_name': 'tmp51upy7_e_RAG_benefits', 'document_id': '4d90e27d-5c78-42a0-a84f-28bd727b0b63'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"Red Hat RAG target audience and customers\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m target\u001b[0m\u001b[33m audience\u001b[0m\u001b[33m and\u001b[0m\u001b[33m customers\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m are\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m End\u001b[0m\u001b[33m users\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Consumers\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m final\u001b[0m\u001b[33m product\u001b[0m\u001b[33m,\u001b[0m\u001b[33m who\u001b[0m\u001b[33m interact\u001b[0m\u001b[33m with\u001b[0m\u001b[33m a\u001b[0m\u001b[33m chat\u001b[0m\u001b[33m G\u001b[0m\u001b[33mPT\u001b[0m\u001b[33m-like\u001b[0m\u001b[33m application\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Builders\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Cre\u001b[0m\u001b[33mators\u001b[0m\u001b[33m and\u001b[0m\u001b[33m configur\u001b[0m\u001b[33mators\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m used\u001b[0m\u001b[33m by\u001b[0m\u001b[33m end\u001b[0m\u001b[33m users\u001b[0m\u001b[33m.\u001b[0m\u001b[33m They\u001b[0m\u001b[33m optimize\u001b[0m\u001b[33m their\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m for\u001b[0m\u001b[33m their\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m end\u001b[0m\u001b[33m users\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m\t\u001b[0m\u001b[33m+\u001b[0m\u001b[33m High\u001b[0m\u001b[33m-c\u001b[0m\u001b[33moder\u001b[0m\u001b[33m builders\u001b[0m\u001b[33m (\u001b[0m\u001b[33mPro\u001b[0m\u001b[33mCode\u001b[0m\u001b[33m):\u001b[0m\u001b[33m Prefer\u001b[0m\u001b[33m SDK\u001b[0m\u001b[33ms\u001b[0m\u001b[33m and\u001b[0m\u001b[33m code\u001b[0m\u001b[33m-based\u001b[0m\u001b[33m solutions\u001b[0m\u001b[33m,\u001b[0m\u001b[33m need\u001b[0m\u001b[33m access\u001b[0m\u001b[33m to\u001b[0m\u001b[33m all\u001b[0m\u001b[33m configurable\u001b[0m\u001b[33m parameters\u001b[0m\u001b[33m via\u001b[0m\u001b[33m APIs\u001b[0m\u001b[33m and\u001b[0m\u001b[33m SDK\u001b[0m\u001b[33ms\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m\t\u001b[0m\u001b[33m+\u001b[0m\u001b[33m Low\u001b[0m\u001b[33m-c\u001b[0m\u001b[33moder\u001b[0m\u001b[33m builders\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Prefer\u001b[0m\u001b[33m UI\u001b[0m\u001b[33m-driven\u001b[0m\u001b[33m workflows\u001b[0m\u001b[33m and\u001b[0m\u001b[33m visual\u001b[0m\u001b[33m tools\u001b[0m\u001b[33m to\u001b[0m\u001b[33m configure\u001b[0m\u001b[33m their\u001b[0m\u001b[33m systems\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Platform\u001b[0m\u001b[33mers\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Build\u001b[0m\u001b[33m,\u001b[0m\u001b[33m maintain\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m secure\u001b[0m\u001b[33m the\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m platform\u001b[0m\u001b[33m and\u001b[0m\u001b[33m APIs\u001b[0m\u001b[33m.\u001b[0m\u001b[33m They\u001b[0m\u001b[33m serve\u001b[0m\u001b[33m both\u001b[0m\u001b[33m builders\u001b[0m\u001b[33m and\u001b[0m\u001b[33m ops\u001b[0m\u001b[33mers\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Ops\u001b[0m\u001b[33mers\u001b[0m\u001b[33m:\u001b[0m\u001b[33m Operational\u001b[0m\u001b[33mize\u001b[0m\u001b[33m and\u001b[0m\u001b[33m automate\u001b[0m\u001b[33m the\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m ML\u001b[0m\u001b[33m life\u001b[0m\u001b[33m cycle\u001b[0m\u001b[33m.\u001b[0m\u001b[33m They\u001b[0m\u001b[33m use\u001b[0m\u001b[33m platform\u001b[0m\u001b[33m APIs\u001b[0m\u001b[33m to\u001b[0m\u001b[33m deploy\u001b[0m\u001b[33m,\u001b[0m\u001b[33m monitor\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m manage\u001b[0m\u001b[33m models\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mNote\u001b[0m\u001b[33m that\u001b[0m\u001b[33m the\u001b[0m\u001b[33m target\u001b[0m\u001b[33m audience\u001b[0m\u001b[33m and\u001b[0m\u001b[33m customers\u001b[0m\u001b[33m of\u001b[0m\u001b[33m Red\u001b[0m\u001b[33m Hat\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m are\u001b[0m\u001b[33m not\u001b[0m\u001b[33m explicitly\u001b[0m\u001b[33m stated\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m search\u001b[0m\u001b[33m results\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m can\u001b[0m\u001b[33m be\u001b[0m\u001b[33m inferred\u001b[0m\u001b[33m from\u001b[0m\u001b[33m the\u001b[0m\u001b[33m context\u001b[0m\u001b[33m and\u001b[0m\u001b[33m the\u001b[0m\u001b[33m information\u001b[0m\u001b[33m provided\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/f79e548b-4078-4d37-bdb6-e82a8d190dee/session/6b9db4ef-121c-46fc-bbe5-bf4772c9aac1/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> What beneficial goals RAG support?\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'RAG beneficial goals'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: Key market use cases. RAC is being adopted across various industries for diverse applications,\\nincluding knowledge question answering, providing accurate answers in customer service using product\\nmanuals or fax. Code generation, retrieving relevant code snippets and documentation to\\nassist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\ncontext. Customer service, improving support accuracy with access to current product information.\\nPersonal assistance, enabling more comprehensive and accurate information from AI assistants.\\nMulti-hub question answering, handling complex multi-step questions through iterative retrieval.\\nLegal applications, retrieving legal documents and case law for reliable legal opinions.\\nGeneral task assistance, aiding users in various tasks requiring information access and decision\\nmaking. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\nis also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\nPopular patience, promoted經 Triологi yokov sources of resources that have tested for\\nimportant יה limit. Online,\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '69a12f52-41d0-4fe2-a7f8-958ca48e4969'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: The benefits of RAC\\nThe retrieval mechanisms built into a RAC architecture allow it to tap into additional data sources beyond an LLM general training.\\nGrounding an LLM on a set of external verifiable facts via RAC supports several beneficial goals.\\nAccuracy\\nRAC provides an LLM with sources it can cite so users can verify these claims.\\nYou can also design a RAC architecture to respond with I don't know if the question is outside the scope of its knowledge.\\nOverall RAC reduces the chances of an LLM sharing incorrect or misleading information as an output and may increase user trust.\\nCost effectiveness\\nRetraining and fine-tuning LLMs costly and time-consuming as it's creating a foundation model to build something like a chatbot from scratch with domain-specific information.\\nWith RAC a user can introduce new data to an LLM as well as swap out or update sources of information by simply uploading a document or file.\\nRAC can also reduce inference codes.\\nLLM queries are expensive, placing demands on your own hardware if you run a local model or running up a mattered bill if you use an external service through an application programming interface.\\nRather than sending an entire reference document to an LLM at once, RAC can send only the most relevant chunks of the reference material, thereby reducing the size of queries and improving efficiency.\\nDeveloper Control\\nCompared to traditional fine-tuning methods, RAC provides a more accessible and straightforward way to get feedback, troubleshoot and fix applications.\\nFor developers, the biggest benefit of RAC architecture is that it lets them take advantage of a stream of domain-specific and up-to-date information.\\nData sovereignty and privacy\\nUsing confidential information to fine-tune an LLM tool has historically been risky as LLMs can reveal information from their training data.\\nRAC offers a solution to these privacy concerns by allowing sensitive data to remain on-premise while still being used to inform a local LLM or a trusted external LLM.\\nRAC architecture can also be set up to restrict sensitive information retrieval to different authorization levels.\\nThat is, certain users can access certain information based on their security clearance levels.\\nThese values can be faced from president to the right of this position where the field you have been claiming that breachesayd 상 and pressing follow certain modules.\\nMetadata: {'file_name': 'tmp51upy7_e_RAG_benefits', 'document_id': 'f928ee9b-f644-4125-8c43-83d9da528dfe'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: RAC vs. Regular LLM Outputs\\nLLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\nAI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\nLLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\nLLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\nLLMs have a knowledge cut-off date, meaning the information they've been trained on doesn't continuously gather updates.\\nAs a result, the resource material can become outdated and no longer relevant.\\nLLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\nImplementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\nThe LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\nLLM and our continental\\nMuch of communication\\nLLMs 트랁\\nMetadata: {'file_name': 'tmpwloo59c6_RAG_vs_Regular_LLM_Output', 'document_id': 'b2b26438-2fe8-47fb-a77c-4360c4be4320'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: Clarifying target audience and user roles. This document clarifies the target audience and user\\nroles for Red Hat Rack project focusing on the distinction between end users and builders.\\nEnd users vs. builders. End users consume the final product.\\nInteract with a chat GPT-like application. Builders create and configure the AI systems\\nused by end users. Configure a Rack backend tweaking parameters for a specific\\nexperience such as chat GPT. We are targeting builders not end users. Builders optimize\\ntheir systems for their specific end users. Builder archetypes. High-coder builders aka\\nProCode prefer SDKs and code-based solutions. They need access to all configurable\\nparameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\nsystem via a UI. Example, LamaStackCliMyRackApp.py-web.\\nLow-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\nsystems. They could benefit from tools like the existing LamaStackPlayground and so on.\\nBuilders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\nand its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\nwhether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.\\nPlatformers focus on building, maintaining, and securing the AI platform and APIs. They serve\\nboth builders for development and opsors for development slash operations, ensuring infrastructure\\nis reliable, scalable, and supports self-service.\\nOpsers. AI ML Ops Engineers. Opsers focus on operationalizing and automating the AI ML life\\ncycle. For example, they use platform APIs to deploy, monitor, and manage models, enabling\\nbuilders models to reach and succeed in production. Opsors work closely with platformers to ensure\\ninfrastructure meets operational needs.\\nIn summary, platformers enable builders and other developers to create systems for end-users.\\nThe Red Hat Rack team focus is on empowering builders with the tools and flexibility they need to\\nbuild the best experience for their end-users.\\nThere are new levers there are new leaders.\\nManyоф methods of lobbying.\\nThere are streams that have owned by AWS company its venir using a light El��게 and construction\\nprotection.\\nMetadata: {'file_name': 'RAG_customers', 'document_id': 'd33413d1-5770-42ab-9851-e4e6c0cb661a'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: I've on my sca beans I've called for OUTON card.\\nI've bought a neighbors and everyone's rules on the container.\\nMetadata: {'file_name': 'RAG_customers', 'document_id': '7f50b0c6-8117-40b9-8774-19c6d1b88af5'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"RAG beneficial goals\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m beneficial\u001b[0m\u001b[33m goals\u001b[0m\u001b[33m supported\u001b[0m\u001b[33m by\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRet\u001b[0m\u001b[33mrie\u001b[0m\u001b[33mval\u001b[0m\u001b[33m-Aug\u001b[0m\u001b[33mmented\u001b[0m\u001b[33m Generation\u001b[0m\u001b[33m)\u001b[0m\u001b[33m are\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Accuracy\u001b[0m\u001b[33m:\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAC\u001b[0m\u001b[33m provides\u001b[0m\u001b[33m an\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m with\u001b[0m\u001b[33m sources\u001b[0m\u001b[33m it\u001b[0m\u001b[33m can\u001b[0m\u001b[33m cite\u001b[0m\u001b[33m so\u001b[0m\u001b[33m users\u001b[0m\u001b[33m can\u001b[0m\u001b[33m verify\u001b[0m\u001b[33m these\u001b[0m\u001b[33m claims\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Cost\u001b[0m\u001b[33m effectiveness\u001b[0m\u001b[33m:\u001b[0m\u001b[33m With\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAC\u001b[0m\u001b[33m,\u001b[0m\u001b[33m a\u001b[0m\u001b[33m user\u001b[0m\u001b[33m can\u001b[0m\u001b[33m introduce\u001b[0m\u001b[33m new\u001b[0m\u001b[33m data\u001b[0m\u001b[33m to\u001b[0m\u001b[33m an\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m as\u001b[0m\u001b[33m well\u001b[0m\u001b[33m as\u001b[0m\u001b[33m swap\u001b[0m\u001b[33m out\u001b[0m\u001b[33m or\u001b[0m\u001b[33m update\u001b[0m\u001b[33m sources\u001b[0m\u001b[33m of\u001b[0m\u001b[33m information\u001b[0m\u001b[33m by\u001b[0m\u001b[33m simply\u001b[0m\u001b[33m uploading\u001b[0m\u001b[33m a\u001b[0m\u001b[33m document\u001b[0m\u001b[33m or\u001b[0m\u001b[33m file\u001b[0m\u001b[33m,\u001b[0m\u001b[33m reducing\u001b[0m\u001b[33m the\u001b[0m\u001b[33m need\u001b[0m\u001b[33m for\u001b[0m\u001b[33m re\u001b[0m\u001b[33mtraining\u001b[0m\u001b[33m and\u001b[0m\u001b[33m fine\u001b[0m\u001b[33m-t\u001b[0m\u001b[33muning\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Developer\u001b[0m\u001b[33m Control\u001b[0m\u001b[33m:\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAC\u001b[0m\u001b[33m provides\u001b[0m\u001b[33m a\u001b[0m\u001b[33m more\u001b[0m\u001b[33m accessible\u001b[0m\u001b[33m and\u001b[0m\u001b[33m straightforward\u001b[0m\u001b[33m way\u001b[0m\u001b[33m to\u001b[0m\u001b[33m get\u001b[0m\u001b[33m feedback\u001b[0m\u001b[33m,\u001b[0m\u001b[33m troub\u001b[0m\u001b[33mleshoot\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m fix\u001b[0m\u001b[33m applications\u001b[0m\u001b[33m,\u001b[0m\u001b[33m allowing\u001b[0m\u001b[33m developers\u001b[0m\u001b[33m to\u001b[0m\u001b[33m take\u001b[0m\u001b[33m advantage\u001b[0m\u001b[33m of\u001b[0m\u001b[33m a\u001b[0m\u001b[33m stream\u001b[0m\u001b[33m of\u001b[0m\u001b[33m domain\u001b[0m\u001b[33m-specific\u001b[0m\u001b[33m and\u001b[0m\u001b[33m up\u001b[0m\u001b[33m-to\u001b[0m\u001b[33m-date\u001b[0m\u001b[33m information\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Data\u001b[0m\u001b[33m sovereignty\u001b[0m\u001b[33m and\u001b[0m\u001b[33m privacy\u001b[0m\u001b[33m:\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAC\u001b[0m\u001b[33m offers\u001b[0m\u001b[33m a\u001b[0m\u001b[33m solution\u001b[0m\u001b[33m to\u001b[0m\u001b[33m privacy\u001b[0m\u001b[33m concerns\u001b[0m\u001b[33m by\u001b[0m\u001b[33m allowing\u001b[0m\u001b[33m sensitive\u001b[0m\u001b[33m data\u001b[0m\u001b[33m to\u001b[0m\u001b[33m remain\u001b[0m\u001b[33m on\u001b[0m\u001b[33m-pre\u001b[0m\u001b[33mmise\u001b[0m\u001b[33m while\u001b[0m\u001b[33m still\u001b[0m\u001b[33m being\u001b[0m\u001b[33m used\u001b[0m\u001b[33m to\u001b[0m\u001b[33m inform\u001b[0m\u001b[33m a\u001b[0m\u001b[33m local\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m or\u001b[0m\u001b[33m a\u001b[0m\u001b[33m trusted\u001b[0m\u001b[33m external\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m,\u001b[0m\u001b[33m with\u001b[0m\u001b[33m the\u001b[0m\u001b[33m ability\u001b[0m\u001b[33m to\u001b[0m\u001b[33m restrict\u001b[0m\u001b[33m sensitive\u001b[0m\u001b[33m information\u001b[0m\u001b[33m retrieval\u001b[0m\u001b[33m to\u001b[0m\u001b[33m different\u001b[0m\u001b[33m authorization\u001b[0m\u001b[33m levels\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThese\u001b[0m\u001b[33m goals\u001b[0m\u001b[33m highlight\u001b[0m\u001b[33m the\u001b[0m\u001b[33m potential\u001b[0m\u001b[33m benefits\u001b[0m\u001b[33m of\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m in\u001b[0m\u001b[33m providing\u001b[0m\u001b[33m accurate\u001b[0m\u001b[33m and\u001b[0m\u001b[33m reliable\u001b[0m\u001b[33m information\u001b[0m\u001b[33m,\u001b[0m\u001b[33m while\u001b[0m\u001b[33m also\u001b[0m\u001b[33m addressing\u001b[0m\u001b[33m concerns\u001b[0m\u001b[33m around\u001b[0m\u001b[33m cost\u001b[0m\u001b[33m,\u001b[0m\u001b[33m developer\u001b[0m\u001b[33m control\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m data\u001b[0m\u001b[33m privacy\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/f79e548b-4078-4d37-bdb6-e82a8d190dee/session/6b9db4ef-121c-46fc-bbe5-bf4772c9aac1/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> Regular LLM output disadvantages\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Args:{'query': 'Regular LLM output disadvantages'}\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:knowledge_search Response:[TextContentItem(text='knowledge_search tool found 5 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: RAC vs. Regular LLM Outputs\\nLLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\nAI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\nLLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\nLLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\nLLMs have a knowledge cut-off date, meaning the information they've been trained on doesn't continuously gather updates.\\nAs a result, the resource material can become outdated and no longer relevant.\\nLLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\nImplementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\nThe LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\nLLM and our continental\\nMuch of communication\\nLLMs 트랁\\nMetadata: {'file_name': 'tmpwloo59c6_RAG_vs_Regular_LLM_Output', 'document_id': 'b2b26438-2fe8-47fb-a77c-4360c4be4320'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: The benefits of RAC\\nThe retrieval mechanisms built into a RAC architecture allow it to tap into additional data sources beyond an LLM general training.\\nGrounding an LLM on a set of external verifiable facts via RAC supports several beneficial goals.\\nAccuracy\\nRAC provides an LLM with sources it can cite so users can verify these claims.\\nYou can also design a RAC architecture to respond with I don't know if the question is outside the scope of its knowledge.\\nOverall RAC reduces the chances of an LLM sharing incorrect or misleading information as an output and may increase user trust.\\nCost effectiveness\\nRetraining and fine-tuning LLMs costly and time-consuming as it's creating a foundation model to build something like a chatbot from scratch with domain-specific information.\\nWith RAC a user can introduce new data to an LLM as well as swap out or update sources of information by simply uploading a document or file.\\nRAC can also reduce inference codes.\\nLLM queries are expensive, placing demands on your own hardware if you run a local model or running up a mattered bill if you use an external service through an application programming interface.\\nRather than sending an entire reference document to an LLM at once, RAC can send only the most relevant chunks of the reference material, thereby reducing the size of queries and improving efficiency.\\nDeveloper Control\\nCompared to traditional fine-tuning methods, RAC provides a more accessible and straightforward way to get feedback, troubleshoot and fix applications.\\nFor developers, the biggest benefit of RAC architecture is that it lets them take advantage of a stream of domain-specific and up-to-date information.\\nData sovereignty and privacy\\nUsing confidential information to fine-tune an LLM tool has historically been risky as LLMs can reveal information from their training data.\\nRAC offers a solution to these privacy concerns by allowing sensitive data to remain on-premise while still being used to inform a local LLM or a trusted external LLM.\\nRAC architecture can also be set up to restrict sensitive information retrieval to different authorization levels.\\nThat is, certain users can access certain information based on their security clearance levels.\\nThese values can be faced from president to the right of this position where the field you have been claiming that breachesayd 상 and pressing follow certain modules.\\nMetadata: {'file_name': 'tmp51upy7_e_RAG_benefits', 'document_id': 'f928ee9b-f644-4125-8c43-83d9da528dfe'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: Key market use cases. RAC is being adopted across various industries for diverse applications,\\nincluding knowledge question answering, providing accurate answers in customer service using product\\nmanuals or fax. Code generation, retrieving relevant code snippets and documentation to\\nassist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\ncontext. Customer service, improving support accuracy with access to current product information.\\nPersonal assistance, enabling more comprehensive and accurate information from AI assistants.\\nMulti-hub question answering, handling complex multi-step questions through iterative retrieval.\\nLegal applications, retrieving legal documents and case law for reliable legal opinions.\\nGeneral task assistance, aiding users in various tasks requiring information access and decision\\nmaking. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\nis also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\nPopular patience, promoted經 Triологi yokov sources of resources that have tested for\\nimportant יה limit. Online,\\nMetadata: {'file_name': 'RAG_use_cases', 'document_id': '69a12f52-41d0-4fe2-a7f8-958ca48e4969'}\\n\", type='text'), TextContentItem(text=\"Result 4\\nContent: Clarifying target audience and user roles. This document clarifies the target audience and user\\nroles for Red Hat Rack project focusing on the distinction between end users and builders.\\nEnd users vs. builders. End users consume the final product.\\nInteract with a chat GPT-like application. Builders create and configure the AI systems\\nused by end users. Configure a Rack backend tweaking parameters for a specific\\nexperience such as chat GPT. We are targeting builders not end users. Builders optimize\\ntheir systems for their specific end users. Builder archetypes. High-coder builders aka\\nProCode prefer SDKs and code-based solutions. They need access to all configurable\\nparameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\nsystem via a UI. Example, LamaStackCliMyRackApp.py-web.\\nLow-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\nsystems. They could benefit from tools like the existing LamaStackPlayground and so on.\\nBuilders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\nand its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\nwhether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.\\nPlatformers focus on building, maintaining, and securing the AI platform and APIs. They serve\\nboth builders for development and opsors for development slash operations, ensuring infrastructure\\nis reliable, scalable, and supports self-service.\\nOpsers. AI ML Ops Engineers. Opsers focus on operationalizing and automating the AI ML life\\ncycle. For example, they use platform APIs to deploy, monitor, and manage models, enabling\\nbuilders models to reach and succeed in production. Opsors work closely with platformers to ensure\\ninfrastructure meets operational needs.\\nIn summary, platformers enable builders and other developers to create systems for end-users.\\nThe Red Hat Rack team focus is on empowering builders with the tools and flexibility they need to\\nbuild the best experience for their end-users.\\nThere are new levers there are new leaders.\\nManyоф methods of lobbying.\\nThere are streams that have owned by AWS company its venir using a light El��게 and construction\\nprotection.\\nMetadata: {'file_name': 'RAG_customers', 'document_id': 'd33413d1-5770-42ab-9851-e4e6c0cb661a'}\\n\", type='text'), TextContentItem(text=\"Result 5\\nContent: I've on my sca beans I've called for OUTON card.\\nI've bought a neighbors and everyone's rules on the container.\\nMetadata: {'file_name': 'RAG_customers', 'document_id': '7f50b0c6-8117-40b9-8774-19c6d1b88af5'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"Regular LLM output disadvantages\". Use them as supporting information only in answering this query.\\n', type='text')]\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mThe\u001b[0m\u001b[33m disadvantages\u001b[0m\u001b[33m of\u001b[0m\u001b[33m regular\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m (\u001b[0m\u001b[33mLarge\u001b[0m\u001b[33m Language\u001b[0m\u001b[33m Model\u001b[0m\u001b[33m)\u001b[0m\u001b[33m output\u001b[0m\u001b[33m are\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Lack\u001b[0m\u001b[33m of\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m information\u001b[0m\u001b[33m:\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m are\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m with\u001b[0m\u001b[33m generally\u001b[0m\u001b[33m available\u001b[0m\u001b[33m data\u001b[0m\u001b[33m,\u001b[0m\u001b[33m but\u001b[0m\u001b[33m might\u001b[0m\u001b[33m not\u001b[0m\u001b[33m include\u001b[0m\u001b[33m the\u001b[0m\u001b[33m specific\u001b[0m\u001b[33m information\u001b[0m\u001b[33m you\u001b[0m\u001b[33m want\u001b[0m\u001b[33m them\u001b[0m\u001b[33m to\u001b[0m\u001b[33m reference\u001b[0m\u001b[33m,\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m an\u001b[0m\u001b[33m internal\u001b[0m\u001b[33m data\u001b[0m\u001b[33m set\u001b[0m\u001b[33m from\u001b[0m\u001b[33m your\u001b[0m\u001b[33m organization\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Knowledge\u001b[0m\u001b[33m cut\u001b[0m\u001b[33m-off\u001b[0m\u001b[33m date\u001b[0m\u001b[33m:\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m have\u001b[0m\u001b[33m a\u001b[0m\u001b[33m knowledge\u001b[0m\u001b[33m cut\u001b[0m\u001b[33m-off\u001b[0m\u001b[33m date\u001b[0m\u001b[33m,\u001b[0m\u001b[33m meaning\u001b[0m\u001b[33m the\u001b[0m\u001b[33m information\u001b[0m\u001b[33m they\u001b[0m\u001b[33m've\u001b[0m\u001b[33m been\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m on\u001b[0m\u001b[33m doesn\u001b[0m\u001b[33m't\u001b[0m\u001b[33m continuously\u001b[0m\u001b[33m gather\u001b[0m\u001b[33m updates\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m the\u001b[0m\u001b[33m resource\u001b[0m\u001b[33m material\u001b[0m\u001b[33m can\u001b[0m\u001b[33m become\u001b[0m\u001b[33m outdated\u001b[0m\u001b[33m and\u001b[0m\u001b[33m no\u001b[0m\u001b[33m longer\u001b[0m\u001b[33m relevant\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Hall\u001b[0m\u001b[33muc\u001b[0m\u001b[33mination\u001b[0m\u001b[33m:\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m are\u001b[0m\u001b[33m eager\u001b[0m\u001b[33m to\u001b[0m\u001b[33m please\u001b[0m\u001b[33m,\u001b[0m\u001b[33m which\u001b[0m\u001b[33m means\u001b[0m\u001b[33m they\u001b[0m\u001b[33m sometimes\u001b[0m\u001b[33m present\u001b[0m\u001b[33m false\u001b[0m\u001b[33m or\u001b[0m\u001b[33m outdated\u001b[0m\u001b[33m information\u001b[0m\u001b[33m,\u001b[0m\u001b[33m also\u001b[0m\u001b[33m known\u001b[0m\u001b[33m as\u001b[0m\u001b[33m halluc\u001b[0m\u001b[33mination\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Limited\u001b[0m\u001b[33m domain\u001b[0m\u001b[33m-specific\u001b[0m\u001b[33m knowledge\u001b[0m\u001b[33m:\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m may\u001b[0m\u001b[33m not\u001b[0m\u001b[33m have\u001b[0m\u001b[33m the\u001b[0m\u001b[33m same\u001b[0m\u001b[33m level\u001b[0m\u001b[33m of\u001b[0m\u001b[33m domain\u001b[0m\u001b[33m-specific\u001b[0m\u001b[33m knowledge\u001b[0m\u001b[33m as\u001b[0m\u001b[33m a\u001b[0m\u001b[33m human\u001b[0m\u001b[33m expert\u001b[0m\u001b[33m,\u001b[0m\u001b[33m which\u001b[0m\u001b[33m can\u001b[0m\u001b[33m lead\u001b[0m\u001b[33m to\u001b[0m\u001b[33m inaccur\u001b[0m\u001b[33macies\u001b[0m\u001b[33m or\u001b[0m\u001b[33m incomplete\u001b[0m\u001b[33m information\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Depend\u001b[0m\u001b[33mence\u001b[0m\u001b[33m on\u001b[0m\u001b[33m training\u001b[0m\u001b[33m data\u001b[0m\u001b[33m:\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33ms\u001b[0m\u001b[33m are\u001b[0m\u001b[33m only\u001b[0m\u001b[33m as\u001b[0m\u001b[33m good\u001b[0m\u001b[33m as\u001b[0m\u001b[33m the\u001b[0m\u001b[33m data\u001b[0m\u001b[33m they\u001b[0m\u001b[33m were\u001b[0m\u001b[33m trained\u001b[0m\u001b[33m on\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m if\u001b[0m\u001b[33m the\u001b[0m\u001b[33m training\u001b[0m\u001b[33m data\u001b[0m\u001b[33m is\u001b[0m\u001b[33m biased\u001b[0m\u001b[33m or\u001b[0m\u001b[33m incomplete\u001b[0m\u001b[33m,\u001b[0m\u001b[33m the\u001b[0m\u001b[33m output\u001b[0m\u001b[33m may\u001b[0m\u001b[33m also\u001b[0m\u001b[33m be\u001b[0m\u001b[33m biased\u001b[0m\u001b[33m or\u001b[0m\u001b[33m incomplete\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mThese\u001b[0m\u001b[33m disadvantages\u001b[0m\u001b[33m highlight\u001b[0m\u001b[33m the\u001b[0m\u001b[33m limitations\u001b[0m\u001b[33m of\u001b[0m\u001b[33m regular\u001b[0m\u001b[33m L\u001b[0m\u001b[33mLM\u001b[0m\u001b[33m output\u001b[0m\u001b[33m and\u001b[0m\u001b[33m the\u001b[0m\u001b[33m potential\u001b[0m\u001b[33m benefits\u001b[0m\u001b[33m of\u001b[0m\u001b[33m using\u001b[0m\u001b[33m R\u001b[0m\u001b[33mAG\u001b[0m\u001b[33m (\u001b[0m\u001b[33mRet\u001b[0m\u001b[33mrie\u001b[0m\u001b[33mval\u001b[0m\u001b[33m-Aug\u001b[0m\u001b[33mmented\u001b[0m\u001b[33m Generation\u001b[0m\u001b[33m)\u001b[0m\u001b[33m to\u001b[0m\u001b[33m improve\u001b[0m\u001b[33m the\u001b[0m\u001b[33m accuracy\u001b[0m\u001b[33m and\u001b[0m\u001b[33m reliability\u001b[0m\u001b[33m of\u001b[0m\u001b[33m AI\u001b[0m\u001b[33m-generated\u001b[0m\u001b[33m content\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://lsd-llama-milvus-service:8321/v1/agents/f79e548b-4078-4d37-bdb6-e82a8d190dee/session/6b9db4ef-121c-46fc-bbe5-bf4772c9aac1/turn \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt> What is the economics condition at Ireland in 2025?\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mI\u001b[0m\u001b[33m don\u001b[0m\u001b[33m’t\u001b[0m\u001b[33m know\u001b[0m\u001b[33m what\u001b[0m\u001b[33m the\u001b[0m\u001b[33m economic\u001b[0m\u001b[33m conditions\u001b[0m\u001b[33m will\u001b[0m\u001b[33m be\u001b[0m\u001b[33m like\u001b[0m\u001b[33m in\u001b[0m\u001b[33m Ireland\u001b[0m\u001b[33m in\u001b[0m\u001b[33m \u001b[0m\u001b[33m202\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://lsd-llama-milvus-service:8321/v1/agents/f79e548b-4078-4d37-bdb6-e82a8d190dee/session/6b9db4ef-121c-46fc-bbe5-bf4772c9aac1 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client import Agent, AgentEventLogger\n",
    "import uuid\n",
    "\n",
    "rag_agent = Agent(\n",
    "    client,\n",
    "    model=\"vllm\",\n",
    "    instructions=\"You are a helpful assistant. Answer the user's question based only on the provided search results. Respond with 'I don’t know' if the information is outside of the scope of your knowledge and not present in the search results.\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"builtin::rag/knowledge_search\",\n",
    "            \"args\": {\"vector_db_ids\": [\"asr-vector-db\"]},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "user_prompts = [\n",
    "    \"List RAG key market use cases\",\n",
    "    \"Name Red Hat RAG target audience and customers\",\n",
    "    \"What beneficial goals RAG support?\",\n",
    "    \"Regular LLM output disadvantages\",\n",
    "    \"What is the economics condition at Ireland in 2025?\",  # Dummy question the model will answer with 'I don’t know' or reason why can't answer\n",
    "]\n",
    "\n",
    "session_id = rag_agent.create_session(session_name=f\"s{uuid.uuid4().hex}\")\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"prompt>\", prompt)\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "        stream=True,\n",
    "    )\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        log.print()\n",
    "\n",
    "# Get session response for further evaluation of RAG metrics\n",
    "session_response = client.agents.session.retrieve(\n",
    "    session_id=session_id,\n",
    "    agent_id=rag_agent.agent_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparation for evaluating RAG models using [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/?h=metrics)\n",
    "\n",
    "- We will use two key metrics to show the performance of the RAG server:\n",
    "    1. [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/) - measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "    2. [Response Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/) - metric measures how relevant a response is to the user input. Higher scores indicate better alignment with the user input, while lower scores are given if the response is incomplete or includes redundant information.\n",
    "\n",
    " - Create .env.dev file and paste there your API Key from [Groq Cloud](https://console.groq.com/home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "with open(\".env.dev\", \"w\") as f:\n",
    "    f.write(\"GROQ_API_KEY=PASTE_YOUR_API_KEY_HERE\")\n",
    "\n",
    "# load env variable\n",
    "load_dotenv(dotenv_path=\".env.dev\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from llama_stack_client.types.agents import Turn\n",
    "\n",
    "# Compile regex pattern once for better performance\n",
    "CONTENT_PATTERN = re.compile(r\"Content:\\s*(.*?)(?=\\nMetadata:|$)\", re.DOTALL)\n",
    "\n",
    "\n",
    "# This function extracts the search results for the trace of each query\n",
    "def extract_retrieved_contexts(turn_object: Turn) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts retrieved contexts from LlamaStack tool execution responses.\n",
    "\n",
    "    Args:\n",
    "        turn_object: A Turn object from LlamaStack containing steps with tool responses\n",
    "\n",
    "    Returns:\n",
    "        List of retrieved context strings for Ragas evaluation\n",
    "    \"\"\"\n",
    "    retrieved_context = []\n",
    "\n",
    "    # Filter tool execution steps first to reduce iterations\n",
    "    tool_steps = [\n",
    "        step for step in turn_object.steps if step.step_type == \"tool_execution\"\n",
    "    ]\n",
    "\n",
    "    for step in tool_steps:\n",
    "        for response in step.tool_responses:\n",
    "            if not response.content or not isinstance(response.content, list):\n",
    "                continue\n",
    "\n",
    "            # Process all valid text items at once\n",
    "            text_items = [\n",
    "                item.text\n",
    "                for item in response.content\n",
    "                if (\n",
    "                    hasattr(item, \"text\")\n",
    "                    and hasattr(item, \"type\")\n",
    "                    and item.type == \"text\"\n",
    "                    and item.text\n",
    "                    and item.text.startswith(\"Result \")\n",
    "                    and \"Content:\" in item.text\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            # Extract content from all valid texts\n",
    "            for text in text_items:\n",
    "                match = CONTENT_PATTERN.search(text)\n",
    "                if match:\n",
    "                    retrieved_context.append(match.group(1).strip())\n",
    "\n",
    "    return retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:datasets:PyTorch version 2.7.1 available.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>List RAG key market use cases</td>\n",
       "      <td>[Key market use cases. RAC is being adopted ac...</td>\n",
       "      <td>RAG (Retrieval-Augmented Generation) key marke...</td>\n",
       "      <td>\\nKey Market Use Cases\\nRAG is being adopted a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Name Red Hat RAG target audience and customers</td>\n",
       "      <td>[Clarifying target audience and user roles. Th...</td>\n",
       "      <td>The target audience and customers of Red Hat R...</td>\n",
       "      <td>\\nClarifying Target Audience and User Roles\\nT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       user_input  \\\n",
       "0                   List RAG key market use cases   \n",
       "1  Name Red Hat RAG target audience and customers   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Key market use cases. RAC is being adopted ac...   \n",
       "1  [Clarifying target audience and user roles. Th...   \n",
       "\n",
       "                                            response  \\\n",
       "0  RAG (Retrieval-Augmented Generation) key marke...   \n",
       "1  The target audience and customers of Red Hat R...   \n",
       "\n",
       "                                           reference  \n",
       "0  \\nKey Market Use Cases\\nRAG is being adopted a...  \n",
       "1  \\nClarifying Target Audience and User Roles\\nT...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import EvaluationDataset\n",
    "\n",
    "samples = []\n",
    "\n",
    "references = [\n",
    "    \"\"\"\n",
    "Key Market Use Cases\n",
    "RAG is being adopted across various industries for diverse applications, including:\n",
    "\n",
    "Knowledge Question Answering: Providing accurate answers in customer service using product manuals or FAQs.\n",
    "\n",
    "Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\n",
    "\n",
    "Recommendation Systems: Enhancing recommendations by providing relevant context.\n",
    "\n",
    "Customer Service: Improving support accuracy with access to current product information.\n",
    "\n",
    "Personal Assistants: Enabling more comprehensive and accurate information from AI assistants.\n",
    "\n",
    "Multi-hop Question Answering: Handling complex, multi-step questions through iterative retrieval.\n",
    "\n",
    "Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\n",
    "\n",
    "General Task Assistance: Aiding users in various tasks requiring information access and decision-making.\n",
    "\n",
    "The rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption, allowing for tailored ad copy and product recommendations.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "Clarifying Target Audience and User Roles\n",
    "This document clarifies the target audience and user roles for our project, focusing on the distinction between end-users and builders.\n",
    "End Users vs. Builders:\n",
    "\n",
    "End Users: Consume the final product (e.g., interact with a ChatGPT-like application).\n",
    "Builders: Create and configure the AI systems used by end-users (e.g., configure a RAG backend, tweaking parameters for a specific experience such as ChatGPT).  We are targeting builders, not end-users. Builders optimize their systems for their specific end-users.\n",
    "\n",
    "Builder Archetypes:\n",
    "\n",
    "High-Coder Builders (aka pro-code): Prefer SDKs and code-based solutions. They need access to all configurable parameters via APIs and SDKs.  They may also want a quick way to \"vibe check\" their RAG system via a UI (e.g., llama-stack-cli my-rag-app.py --web).\n",
    "\n",
    "Low-Coder Builders (no/low-code): Prefer UI-driven workflows and visual tools to configure their systems.  They could benefit from tools like the existing llama-stack playground.\n",
    "\n",
    "Builders vs. Platformers vs. Opsers:\n",
    "\n",
    "Builders (AI Engineers/AI Devs): Use the platform and its primitives to build AI systems.  Their skillset and the complexity of their tasks determine whether they are considered AI Engineers or AI Devs.\n",
    "\n",
    "Platformers (AI Platform Engineers): Platformers focus on building, maintaining, and securing the AI platform and APIs. They serve both Builders (for development) and Opsers (for deployment/operations), ensuring infrastructure is reliable, scalable, and supports self-service.\n",
    "\n",
    "Opsers (AI/MLOps Engineers): Opsers focus on operationalizing and automating the AI/ML  lifecycle. For example, they use platform APIs to deploy, monitor, and manage models, enabling Builders' models to reach and succeed in production. Opsers work closely with Platformers to ensure infrastructure meets operational needs.\n",
    "\n",
    "In summary:\n",
    "\n",
    "Platformers enable builders, and builders create systems for end-users.  Our focus is on empowering builders with the tools and flexibility they need to build the best experiences for their end-users.\"\"\",\n",
    "]\n",
    "\n",
    "# Constructing a Ragas EvaluationDataset\n",
    "for i, turn in enumerate(session_response.turns[:2]):\n",
    "    samples.append(\n",
    "        {\n",
    "            \"user_input\": turn.input_messages[0].content,\n",
    "            \"response\": turn.output_message.content,\n",
    "            \"reference\": references[i],\n",
    "            \"retrieved_contexts\": extract_retrieved_contexts(turn),\n",
    "        }\n",
    "    )\n",
    "\n",
    "ragas_eval_dataset = EvaluationDataset.from_list(samples)\n",
    "ragas_eval_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prerequisites for RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: ibm-granite/granite-embedding-125m-english\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved contexts for the first prompt: ['Key market use cases. RAC is being adopted across various industries for diverse applications,\\nincluding knowledge question answering, providing accurate answers in customer service using product\\nmanuals or fax. Code generation, retrieving relevant code snippets and documentation to\\nassist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\ncontext. Customer service, improving support accuracy with access to current product information.\\nPersonal assistance, enabling more comprehensive and accurate information from AI assistants.\\nMulti-hub question answering, handling complex multi-step questions through iterative retrieval.\\nLegal applications, retrieving legal documents and case law for reliable legal opinions.\\nGeneral task assistance, aiding users in various tasks requiring information access and decision\\nmaking. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\nis also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\nPopular patience, promoted經 Triологi yokov sources of resources that have tested for\\nimportant יה limit. Online,', \"The benefits of RAC\\nThe retrieval mechanisms built into a RAC architecture allow it to tap into additional data sources beyond an LLM general training.\\nGrounding an LLM on a set of external verifiable facts via RAC supports several beneficial goals.\\nAccuracy\\nRAC provides an LLM with sources it can cite so users can verify these claims.\\nYou can also design a RAC architecture to respond with I don't know if the question is outside the scope of its knowledge.\\nOverall RAC reduces the chances of an LLM sharing incorrect or misleading information as an output and may increase user trust.\\nCost effectiveness\\nRetraining and fine-tuning LLMs costly and time-consuming as it's creating a foundation model to build something like a chatbot from scratch with domain-specific information.\\nWith RAC a user can introduce new data to an LLM as well as swap out or update sources of information by simply uploading a document or file.\\nRAC can also reduce inference codes.\\nLLM queries are expensive, placing demands on your own hardware if you run a local model or running up a mattered bill if you use an external service through an application programming interface.\\nRather than sending an entire reference document to an LLM at once, RAC can send only the most relevant chunks of the reference material, thereby reducing the size of queries and improving efficiency.\\nDeveloper Control\\nCompared to traditional fine-tuning methods, RAC provides a more accessible and straightforward way to get feedback, troubleshoot and fix applications.\\nFor developers, the biggest benefit of RAC architecture is that it lets them take advantage of a stream of domain-specific and up-to-date information.\\nData sovereignty and privacy\\nUsing confidential information to fine-tune an LLM tool has historically been risky as LLMs can reveal information from their training data.\\nRAC offers a solution to these privacy concerns by allowing sensitive data to remain on-premise while still being used to inform a local LLM or a trusted external LLM.\\nRAC architecture can also be set up to restrict sensitive information retrieval to different authorization levels.\\nThat is, certain users can access certain information based on their security clearance levels.\\nThese values can be faced from president to the right of this position where the field you have been claiming that breachesayd 상 and pressing follow certain modules.\", \"RAC vs. Regular LLM Outputs\\nLLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\nAI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\nLLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\nLLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\nLLMs have a knowledge cut-off date, meaning the information they've been trained on doesn't continuously gather updates.\\nAs a result, the resource material can become outdated and no longer relevant.\\nLLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\nImplementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\nThe LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\nLLM and our continental\\nMuch of communication\\nLLMs 트랁\", 'Clarifying target audience and user roles. This document clarifies the target audience and user\\nroles for Red Hat Rack project focusing on the distinction between end users and builders.\\nEnd users vs. builders. End users consume the final product.\\nInteract with a chat GPT-like application. Builders create and configure the AI systems\\nused by end users. Configure a Rack backend tweaking parameters for a specific\\nexperience such as chat GPT. We are targeting builders not end users. Builders optimize\\ntheir systems for their specific end users. Builder archetypes. High-coder builders aka\\nProCode prefer SDKs and code-based solutions. They need access to all configurable\\nparameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\nsystem via a UI. Example, LamaStackCliMyRackApp.py-web.\\nLow-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\nsystems. They could benefit from tools like the existing LamaStackPlayground and so on.\\nBuilders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\nand its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\nwhether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.\\nPlatformers focus on building, maintaining, and securing the AI platform and APIs. They serve\\nboth builders for development and opsors for development slash operations, ensuring infrastructure\\nis reliable, scalable, and supports self-service.\\nOpsers. AI ML Ops Engineers. Opsers focus on operationalizing and automating the AI ML life\\ncycle. For example, they use platform APIs to deploy, monitor, and manage models, enabling\\nbuilders models to reach and succeed in production. Opsors work closely with platformers to ensure\\ninfrastructure meets operational needs.\\nIn summary, platformers enable builders and other developers to create systems for end-users.\\nThe Red Hat Rack team focus is on empowering builders with the tools and flexibility they need to\\nbuild the best experience for their end-users.\\nThere are new levers there are new leaders.\\nManyоф methods of lobbying.\\nThere are streams that have owned by AWS company its venir using a light El��게 and construction\\nprotection.', 'However, they can also be found on the homepage in journalism category from the provincial levels, starting the nationalvik function\\nat the GZ is a service where we continue andём back in regulators where you are sacrificing data now.']\n",
      "\n",
      "Retrieved contexts for the second prompt: ['Clarifying target audience and user roles. This document clarifies the target audience and user\\nroles for Red Hat Rack project focusing on the distinction between end users and builders.\\nEnd users vs. builders. End users consume the final product.\\nInteract with a chat GPT-like application. Builders create and configure the AI systems\\nused by end users. Configure a Rack backend tweaking parameters for a specific\\nexperience such as chat GPT. We are targeting builders not end users. Builders optimize\\ntheir systems for their specific end users. Builder archetypes. High-coder builders aka\\nProCode prefer SDKs and code-based solutions. They need access to all configurable\\nparameters via APIs and SDKs. They may also want a quick way to wipe check their Rack\\nsystem via a UI. Example, LamaStackCliMyRackApp.py-web.\\nLow-coder builders. No low-code. Prefer UI-driven workflows and visual tools to configure their\\nsystems. They could benefit from tools like the existing LamaStackPlayground and so on.\\nBuilders vs. Platformers vs. Opsers. Builders, AI Engineers and AI Devs. They use the platform\\nand its primitives to build AI systems. Their skill set and the complexity of their tasks determine\\nwhether they are considered AI engineers or AI Devs. Platformers. AI Platform Engineers.\\nPlatformers focus on building, maintaining, and securing the AI platform and APIs. They serve\\nboth builders for development and opsors for development slash operations, ensuring infrastructure\\nis reliable, scalable, and supports self-service.\\nOpsers. AI ML Ops Engineers. Opsers focus on operationalizing and automating the AI ML life\\ncycle. For example, they use platform APIs to deploy, monitor, and manage models, enabling\\nbuilders models to reach and succeed in production. Opsors work closely with platformers to ensure\\ninfrastructure meets operational needs.\\nIn summary, platformers enable builders and other developers to create systems for end-users.\\nThe Red Hat Rack team focus is on empowering builders with the tools and flexibility they need to\\nbuild the best experience for their end-users.\\nThere are new levers there are new leaders.\\nManyоф methods of lobbying.\\nThere are streams that have owned by AWS company its venir using a light El��게 and construction\\nprotection.', 'Key market use cases. RAC is being adopted across various industries for diverse applications,\\nincluding knowledge question answering, providing accurate answers in customer service using product\\nmanuals or fax. Code generation, retrieving relevant code snippets and documentation to\\nassist in code creation. Recommendation systems, enhancing recommendations by providing relevant\\ncontext. Customer service, improving support accuracy with access to current product information.\\nPersonal assistance, enabling more comprehensive and accurate information from AI assistants.\\nMulti-hub question answering, handling complex multi-step questions through iterative retrieval.\\nLegal applications, retrieving legal documents and case law for reliable legal opinions.\\nGeneral task assistance, aiding users in various tasks requiring information access and decision\\nmaking. The rising demand for hyper-personalized content in areas like marketing and e-commerce\\nis also a significant driver for RAC adoption, allowing for tailored ad copy and product recommendations.\\nPopular patience, promoted經 Triологi yokov sources of resources that have tested for\\nimportant יה limit. Online,', \"The benefits of RAC\\nThe retrieval mechanisms built into a RAC architecture allow it to tap into additional data sources beyond an LLM general training.\\nGrounding an LLM on a set of external verifiable facts via RAC supports several beneficial goals.\\nAccuracy\\nRAC provides an LLM with sources it can cite so users can verify these claims.\\nYou can also design a RAC architecture to respond with I don't know if the question is outside the scope of its knowledge.\\nOverall RAC reduces the chances of an LLM sharing incorrect or misleading information as an output and may increase user trust.\\nCost effectiveness\\nRetraining and fine-tuning LLMs costly and time-consuming as it's creating a foundation model to build something like a chatbot from scratch with domain-specific information.\\nWith RAC a user can introduce new data to an LLM as well as swap out or update sources of information by simply uploading a document or file.\\nRAC can also reduce inference codes.\\nLLM queries are expensive, placing demands on your own hardware if you run a local model or running up a mattered bill if you use an external service through an application programming interface.\\nRather than sending an entire reference document to an LLM at once, RAC can send only the most relevant chunks of the reference material, thereby reducing the size of queries and improving efficiency.\\nDeveloper Control\\nCompared to traditional fine-tuning methods, RAC provides a more accessible and straightforward way to get feedback, troubleshoot and fix applications.\\nFor developers, the biggest benefit of RAC architecture is that it lets them take advantage of a stream of domain-specific and up-to-date information.\\nData sovereignty and privacy\\nUsing confidential information to fine-tune an LLM tool has historically been risky as LLMs can reveal information from their training data.\\nRAC offers a solution to these privacy concerns by allowing sensitive data to remain on-premise while still being used to inform a local LLM or a trusted external LLM.\\nRAC architecture can also be set up to restrict sensitive information retrieval to different authorization levels.\\nThat is, certain users can access certain information based on their security clearance levels.\\nThese values can be faced from president to the right of this position where the field you have been claiming that breachesayd 상 and pressing follow certain modules.\", \"RAC vs. Regular LLM Outputs\\nLLMs use machine learning and natural language processing NLP techniques to understand and generate human language for AI inference.\\nAI inference is the operational phase of AI where the model is able to apply the learning from training and apply it to real-world solutions and situations.\\nLLMs can be incredibly valuable for communication and data processing, but they have disadvantages too.\\nLLMs are trained with generally available data but might not include the specific information you want them to reference, such as an internal data set from your organization.\\nLLMs have a knowledge cut-off date, meaning the information they've been trained on doesn't continuously gather updates.\\nAs a result, the resource material can become outdated and no longer relevant.\\nLLMs are eager to please, which means they sometimes present false or outdated information, also known as hallucination.\\nImplementing RAC architecture into an LLM-based question-answering system provides a line of communication between an LLM and your chosen additional knowledge sources.\\nThe LLM is able to cross-reference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.\\nLLM and our continental\\nMuch of communication\\nLLMs 트랁\", 'However, they can also be found on the homepage in journalism category from the provincial levels, starting the nationalvik function\\nat the GZ is a service where we continue andём back in regulators where you are sacrificing data now.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ResponseRelevancy,\n",
    ")\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from langchain_groq import ChatGroq\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Wrap the Groq LLM for use with Ragas\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# Using HuggingFace embeddings as a free alternative\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"ibm-granite/granite-embedding-125m-english\"\n",
    ")\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings_model)\n",
    "\n",
    "\n",
    "# references for both prompts\n",
    "reference_for_first_prompt = samples[0][\"reference\"]\n",
    "reference_for_second_prompt = samples[1][\"reference\"]\n",
    "\n",
    "# inputs for both prompts\n",
    "user_input_for_first_prompt = samples[0][\"user_input\"]\n",
    "user_input_for_second_prompt = samples[1][\"user_input\"]\n",
    "\n",
    "# responses for both prompts\n",
    "response_for_first_prompt = samples[0][\"response\"]\n",
    "response_for_second_prompt = samples[1][\"response\"]\n",
    "\n",
    "# reference lists for both prompts\n",
    "reference_list_for_first_prompt = [\n",
    "    line.strip() for line in reference_for_first_prompt.strip().split(\"\\n\")\n",
    "]\n",
    "reference_list_for_second_prompt = [\n",
    "    line.strip() for line in reference_for_second_prompt.strip().split(\"\\n\")\n",
    "]\n",
    "\n",
    "# Retrieved contexts for both prompts\n",
    "retrieved_contexts_for_first_prompt = samples[0][\"retrieved_contexts\"]\n",
    "retrieved_contexts_for_second_prompt = samples[1][\"retrieved_contexts\"]\n",
    "\n",
    "print(\n",
    "    f\"Retrieved contexts for the first prompt: {retrieved_contexts_for_first_prompt}\\n\"\n",
    ")\n",
    "print(\n",
    "    f\"Retrieved contexts for the second prompt: {retrieved_contexts_for_second_prompt}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Faithfulness Score for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness score for prompt 'List RAG key market use cases': 1.0\n"
     ]
    }
   ],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "    user_input=user_input_for_first_prompt,\n",
    "    response=response_for_first_prompt,\n",
    "    retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    ")\n",
    "faithfulness_scorer = Faithfulness(llm=evaluator_llm)\n",
    "faithfulness_score_for_first_prompt = await faithfulness_scorer.single_turn_ascore(\n",
    "    first_prompt_turn\n",
    ")\n",
    "print(\n",
    "    f\"Faithfulness score for prompt '{user_prompts[0]}': {faithfulness_score_for_first_prompt}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 9.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 38.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness score for prompt 'Name Red Hat RAG target audience and customers': 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "second_prompt_turn = SingleTurnSample(\n",
    "    user_input=user_input_for_second_prompt,\n",
    "    response=response_for_second_prompt,\n",
    "    retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    ")\n",
    "faithfulness_score_for_second_prompt = await faithfulness_scorer.single_turn_ascore(\n",
    "    second_prompt_turn\n",
    ")\n",
    "print(\n",
    "    f\"Faithfulness score for prompt '{user_prompts[1]}': {faithfulness_score_for_second_prompt}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Response Relevancy for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Relevancy score for prompt 'List RAG key market use cases': 0.9039218462290007\n"
     ]
    }
   ],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "    user_input=user_input_for_first_prompt,\n",
    "    response=response_for_first_prompt,\n",
    "    retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    ")\n",
    "response_relevancy_scorer = ResponseRelevancy(\n",
    "    llm=evaluator_llm, embeddings=evaluator_embeddings\n",
    ")\n",
    "response_relevancy_score_for_first_prompt = (\n",
    "    await response_relevancy_scorer.single_turn_ascore(first_prompt_turn)\n",
    ")\n",
    "print(\n",
    "    f\"Response Relevancy score for prompt '{user_prompts[0]}': {response_relevancy_score_for_first_prompt}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Relevancy score for prompt 'Name Red Hat RAG target audience and customers': 0.9860429489968419\n"
     ]
    }
   ],
   "source": [
    "second_prompt_turn = SingleTurnSample(\n",
    "    user_input=user_input_for_second_prompt,\n",
    "    response=response_for_second_prompt,\n",
    "    retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    ")\n",
    "response_relevancy_score_for_second_prompt = (\n",
    "    await response_relevancy_scorer.single_turn_ascore(second_prompt_turn)\n",
    ")\n",
    "print(\n",
    "    f\"Response Relevancy score for prompt '{user_prompts[1]}': {response_relevancy_score_for_second_prompt}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
